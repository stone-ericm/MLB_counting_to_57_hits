{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5ffe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pybaseball\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', 10)\n",
    "import numpy as np\n",
    "import statsapi\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from api_keys import visual_crossing\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pybaseball import statcast, statcast_pitcher, statcast_batter, playerid_lookup\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', 10)\n",
    "import numpy as np\n",
    "import statsapi\n",
    "import pybaseball\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, plot_confusion_matrix, precision_score, plot_roc_curve, make_scorer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import operator\n",
    "import math\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3cd6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "035d382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_UTC_to_local(row, venue_coords):\n",
    "    venue_name = row['venue_name']\n",
    "    game_datetime = row['game_datetime']\n",
    "    from_zone = tz.gettz('UTC')\n",
    "\n",
    "    locator = Nominatim(user_agent='myGeocoder')\n",
    "    try:\n",
    "        city = venue_coords[venue_coords['NAME'] == row['venue_name']]['CITY'].iloc[0]\n",
    "        lat = venue_coords[venue_coords['NAME'] == row['venue_name']]['Latitude'].iloc[0]\n",
    "        lon = venue_coords[venue_coords['NAME'] == row['venue_name']]['Longitude'].iloc[0]\n",
    "        alt = venue_coords[venue_coords['NAME'] == row['venue_name']]['Altitude'].iloc[0]\n",
    "    except NameError:\n",
    "        raise IndexError(venue, index)\n",
    "    if city == 'Tokyo':\n",
    "        to_zone = tz.gettz('Asia/Tokyo')\n",
    "    elif city == 'London':\n",
    "        to_zone = tz.gettz('Europe/London')\n",
    "    elif city in ['San Francisco',\n",
    "                        'Oakland',\n",
    "                        'Phoenix',\n",
    "                        'Seattle',\n",
    "                        'Los Angeles',\n",
    "                        'San Diego',\n",
    "                        'Anaheim']:\n",
    "        to_zone = tz.gettz('America/Los_Angeles')\n",
    "    elif city == 'Denver':\n",
    "        to_zone = tz.gettz('America/Denver')\n",
    "    elif city in ['Minneapolis',\n",
    "                        'Milwaukee',\n",
    "                        'Chicago',\n",
    "                        'St. Louis',\n",
    "                        'Arlington',\n",
    "                        'Kansas City',\n",
    "                        'Houston',\n",
    "                        'Monterrey',\n",
    "                        'Omaha',\n",
    "                        'Dyersville']:\n",
    "        to_zone = tz.gettz('America/Chicago')\n",
    "    elif city in ['Buffalo',\n",
    "                        'Detroit',\n",
    "                        'Cincinnati',\n",
    "                        'Pittsburgh',\n",
    "                        'Tampa Bay',\n",
    "                        'Philadelphia',\n",
    "                        'Atlanta',\n",
    "                        'New York',\n",
    "                        'Washington',\n",
    "                        'Cleveland',\n",
    "                        'Miami',\n",
    "                        'Boston',\n",
    "                        'Baltimore',\n",
    "                        'Toronto',\n",
    "                        'Williamsport',\n",
    "                        'Dunedin',\n",
    "                        'St. Petersburg']:\n",
    "        to_zone = tz.gettz('America/New_York')\n",
    "    else:\n",
    "        raise NameError(venue_name, city)\n",
    "    utc = datetime.strptime(game_datetime, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "    local = utc.astimezone(to_zone)\n",
    "\n",
    "    return(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3ed105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_ending_events = np.array(['field_out',\n",
    "                        'strikeout',\n",
    "                        'single',\n",
    "                        'walk',\n",
    "                        'double',\n",
    "                        'home_run',\n",
    "                        'force_out',\n",
    "                        'grounded_into_double_play',\n",
    "                        'hit_by_pitch',\n",
    "                        'field_error',\n",
    "                        'sac_fly',\n",
    "                        'triple',\n",
    "                        'sac_bunt',\n",
    "                        'fielders_choice',\n",
    "                        'double_play',\n",
    "                        'fielders_choice_out',\n",
    "                        'strikeout_double_play',\n",
    "                        'catcher_interf',\n",
    "                        'sac_fly_double_play',\n",
    "                        'triple_play',\n",
    "                        'sac_bunt_double_play'])\n",
    "\n",
    "\n",
    "hit_events = np.array(['single',\n",
    "            'double',\n",
    "            'home_run',\n",
    "            'triple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66fddd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_date_weather_data(game_pk_df):\n",
    "    venue_coords = pd.read_csv('Parks.csv')\n",
    "    data_collection = {}\n",
    "\n",
    "    try:\n",
    "        game_pk_df['coordinates'] = game_pk_df['venue_name'].apply(lambda x: ','.join([str(venue_coords[venue_coords['NAME'] == x]['Latitude'].iloc[0]), str(venue_coords[venue_coords['NAME'] == x]['Longitude'].iloc[0])]))\n",
    "    except NameError:\n",
    "        raise IndexError(venue, index)\n",
    "\n",
    "    game_pk_df = game_pk_df.sort_values(['game_datetime'])\n",
    "\n",
    "    \n",
    "\n",
    "    game_pk_df['local_datetime'] = game_pk_df.apply(lambda row: convert_UTC_to_local(row, venue_coords), axis=1)\n",
    "    game_pk_df['local_datetime'] = game_pk_df['local_datetime'].apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "    \n",
    "\n",
    "    from api_keys import visual_crossing\n",
    "    import requests\n",
    "\n",
    "    weather_df = pd.DataFrame()\n",
    "    for index, row in game_pk_df.iterrows():\n",
    "\n",
    "#         check if key is past or future\n",
    "        if datetime.strptime(row['game_datetime'], \"%Y-%m-%dT%H:%M:%SZ\") > datetime.utcnow():\n",
    "            URL = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast?locations={row[\"coordinates\"]}&aggregateHours=1&unitGroup=us&forecastDays=1&shortColumnNames=true&contentType=json&key={visual_crossing}'\n",
    "\n",
    "        else:\n",
    "            URL = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?&aggregateHours=1&startDateTime={row[\"local_datetime\"]}&endDateTime={row[\"local_datetime\"]}&unitGroup=us&contentType=json&location={row[\"coordinates\"]}&key={visual_crossing}'\n",
    "        global api_counter\n",
    "        if api_counter >= 1000:\n",
    "            print('Prepreprocessing will continue at ' + (datetime.now()+timedelta(seconds = 86400)).strftime('%H:%M:%S'))\n",
    "            sleep(86400)\n",
    "            api_counter = 0\n",
    "        try:\n",
    "            api_counter += 1\n",
    "            response = requests.get(URL)\n",
    "            data = response.json()\n",
    "            if data['locations'][row['coordinates']]['values'][0]['temp'] == None:\n",
    "                URL = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/forecast?locations={row[\"coordinates\"]}&aggregateHours=1&unitGroup=us&forecastDays=1&shortColumnNames=true&contentType=json&key={visual_crossing}'\n",
    "                api_counter += 1\n",
    "                response = requests.get(URL)\n",
    "                data = response.json()\n",
    "                \n",
    "            if len(data['locations'][row['coordinates']]['values']) > 1:\n",
    "                for forecast in data['locations'][row['coordinates']]['values']:\n",
    "                    \n",
    "                    if forecast['datetimeStr'][:13] == row['local_datetime'][:13]:\n",
    "                        values = forecast\n",
    "                        values['datetimeStr'] = row['local_datetime']\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                values = data['locations'][row['coordinates']]['values'][0]\n",
    "                values['datetimeStr'] = row['local_datetime']\n",
    "            data_collection[URL] = data\n",
    "            weather_df = weather_df.append({**{'coordinates': row['coordinates']}, **values}, ignore_index=True)\n",
    "    \n",
    "        except:\n",
    "            raise IndexError(index, row, response, response.json())\n",
    "    weather_df = weather_df.rename(columns={'datetimeStr': 'local_datetime'})\n",
    "\n",
    "\n",
    "    games_and_weather = pd.merge(\n",
    "        game_pk_df,\n",
    "        weather_df,\n",
    "        how=\"left\",\n",
    "        on=None,\n",
    "        left_on=['local_datetime', 'coordinates'],\n",
    "        right_on=['local_datetime', 'coordinates'],\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        sort=False,\n",
    "        suffixes=(\"_gpk\", \"_wdf\"),\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "    \n",
    "    return(games_and_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "299c235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statcast_importer(year = datetime.now().year):\n",
    "    if year == datetime.now().year:\n",
    "        end = (datetime.now()-timedelta(weeks=1)).strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        end = str(year) + '-12-31'\n",
    "    \n",
    "    start = str(year) + '-01-01'\n",
    "    df = pybaseball.statcast(str(year) + '-01-01', (datetime.now()-timedelta(weeks=1)).strftime(\"%Y-%m-%d\"))\n",
    "    df = df[df['game_type'] == 'R']\n",
    "    print('game types done')\n",
    "    df['game_date'] = pd.to_datetime(df['game_date'], format='%Y-%m-%d')\n",
    "    print('datetime done')\n",
    "    df['events'] = df['events'].apply(lambda x: 1 if x in hit_events else 0)\n",
    "    print('events done')\n",
    "    df['batter_righty'] = df['stand'].apply(lambda x: 1 if x == 'R' else 0)\n",
    "    print('stand done')\n",
    "    df['pitcher_righty'] = df['p_throws'].apply(lambda x: 1 if x == 'R' else 0)\n",
    "    print('pitcher done')\n",
    "    df['bottom'] = df['inning_topbot'].apply(lambda x: 1 if x == 'Bot' else 0)\n",
    "    print('inning done')\n",
    "    df = df.select_dtypes(exclude=['object'])\n",
    "    print('dropping columns done')\n",
    "\n",
    "\n",
    "    df.to_csv(str(year)+'_statcast_pbp.csv')\n",
    "    return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cac8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably best off grabbing all the data for each player and trimming it myself - speed up, cut down on api calls\n",
    "\n",
    "    # instead of career stats we'll go off last two years to bias towards recency\n",
    "def derived_cumulative_stats_v2(player, stats_type, start_date=None, end_date=datetime.strptime('2021-06-30', '%Y-%m-%d')):\n",
    "    \n",
    "    relevant_years = [str(end_date.year), str(end_date.year-1), str(end_date.year-2)]    \n",
    "    \n",
    "    history = pd.concat([\n",
    "                pd.read_csv(relevant_years[0] + '_statcast_pbp.csv'),\n",
    "                pd.read_csv(relevant_years[1] + '_statcast_pbp.csv'),\n",
    "               pd.read_csv(relevant_years[2] + '_statcast_pbp.csv')\n",
    "                ], ignore_index=True)\n",
    "\n",
    "    if start_date == None:\n",
    "        start_date = end_date-timedelta(weeks=52)\n",
    "    if type(player) == str:\n",
    "        player = player.split(' ')\n",
    "        player.reverse()\n",
    "        try:\n",
    "            player_info = pybaseball.playerid_lookup(player[0], player[1])\n",
    "            if len(player_info) >1:\n",
    "                player_info = player_info.iloc[0]\n",
    "                player = player_info['key_mlbam']\n",
    "            elif player_info.empty:\n",
    "                player_info = pybaseball.playerid_lookup(player[0], player[1], fuzzy=True).iloc[0]\n",
    "                player = player_info['key_mlbam']\n",
    "            else:\n",
    "                try:\n",
    "                    player = player_info.iloc[0]['key_mlbam']\n",
    "                except:\n",
    "                    raise NameError(player_info, player)\n",
    "        except:\n",
    "            raise NameError(player)\n",
    "            \n",
    "    history = history[history['game_type'] == 'R']\n",
    "    history = history[(history['game_date'] >= start_date) & (history['game_date'] <= end_date)]\n",
    "\n",
    "    if stats_type == 'pitcher':\n",
    "        try:\n",
    "            history = history[history['pitcher'] == player]\n",
    "        except:\n",
    "            raise NameError(player)\n",
    "\n",
    "    #   PITCH METRICS\n",
    "        pitches = {}\n",
    "        pitches['pitch_hand'] = statsapi.player_stat_data(player)['pitch_hand']\n",
    "        \n",
    "    #     filter history down to PA-enders\n",
    "        history = history[history['events'].isin([1, 0])]\n",
    "\n",
    "    #    PLAYER METRICS\n",
    "        games = history['game_pk'].unique()\n",
    "        at_bat_list = []\n",
    "        num_hits_list = []\n",
    "        for game in games:\n",
    "            game = history[history['game_pk'] == game]\n",
    "            inning = game['inning'].max()\n",
    "            at_bats = len(game)\n",
    "            at_bat_list.append(at_bats)\n",
    "            for i in range(1, inning+1):\n",
    "                num_hits = len(game[(game['inning'] == i) & (game['events'] == 1)])\n",
    "                num_hits_list.append(num_hits)\n",
    "        pitches['games_played_last_2_years_pitcher'] = len(games)\n",
    "        pitches['avg_PAs_per_apparence_pitcher'] = np.array(at_bat_list).mean()\n",
    "        pitches['avg_hits_per_inning'] = np.array(num_hits_list).mean()\n",
    "\n",
    "#         L/R splits\n",
    "        left_pitcher = history[history['batter_righty'] == 0]\n",
    "        right_pitcher = history[history['batter_righty'] == 1]\n",
    "        l_pas = len(left_pitcher)\n",
    "        r_pas = len(right_pitcher)\n",
    "        l_hits = len(left_pitcher[(history['events'] == 1)])\n",
    "        r_hits = len(right_pitcher[(history['events'] == 1)])\n",
    "\n",
    "        if l_pas > 0 and r_pas > 0:\n",
    "            pitches['H/PA_pitcher'] = (l_hits + r_hits)/(l_pas + r_pas)\n",
    "            pitches['against_lefties_H/PA'] = (l_hits)/(l_pas)\n",
    "            pitches['against_righties_H/PA'] = (r_hits)/(r_pas)\n",
    "        elif l_pas > 0:\n",
    "            pitches['H/PA_pitcher'] = (l_hits + r_hits)/(l_pas + r_pas)\n",
    "            pitches['against_lefties_H/PA'] = (l_hits)/(l_pas)\n",
    "            pitches['against_righties_H/PA'] = 0\n",
    "        elif r_pas > 0:\n",
    "            pitches['H/PA_pitcher'] = (l_hits + r_hits)/(l_pas + r_pas)\n",
    "            pitches['against_lefties_H/PA'] = 0\n",
    "            pitches['against_righties_H/PA'] = (r_hits)/(r_pas)\n",
    "        else:\n",
    "            pitches['H/PA_pitcher'] = 0\n",
    "            pitches['against_lefties_H/PA'] = 0\n",
    "            pitches['against_righties_H/PA'] = 0\n",
    "        return(pitches)\n",
    "\n",
    "    \n",
    "    elif stats_type == 'batter':\n",
    "        history = history[history['batter'] == player]\n",
    "\n",
    "\n",
    "#     filter history down to PA-enders\n",
    "        history = history[history['events'].isin([1, 0])]\n",
    "\n",
    "#     H/PA per pitch type\n",
    "#     K/PA per pitch type\n",
    "#     H/PA vs righties, lefties\n",
    "#     PA/G\n",
    "#     avg_launch_angle\n",
    "#     avg_launch_speed\n",
    "#     xBA based on estimated_ba_using_speedangle\n",
    "\n",
    "\n",
    "\n",
    "        # pitches = history['pitch_type'].unique()\n",
    "        games = history['game_pk'].unique()\n",
    "        pas_list = []\n",
    "\n",
    "        batter = {}\n",
    "\n",
    "        for game in games:\n",
    "            pas = len(history[history['game_pk'] == game])\n",
    "            # print(pas)\n",
    "            pas_list.append(pas)\n",
    "        batter['games_played_last_2_years_batter'] = len(games)\n",
    "        batter['PA/G_batter'] = np.array(pas_list).mean()\n",
    "\n",
    "#         righty/lefty split\n",
    "        pitcher_right = history[history['pitcher_righty'] == 1]\n",
    "        pas_r = len(pitcher_right)\n",
    "        hits_r = len(pitcher_right[pitcher_right['events'] == 1])\n",
    "\n",
    "        if pas_r > 0:\n",
    "            batter['H/PA_against_R'] = hits_r/pas_r\n",
    "        else:\n",
    "            batter['H/PA_against_R'] = 0\n",
    "\n",
    "        pitcher_left = history[history['pitcher_righty'] == 0]\n",
    "        pas_l = len(pitcher_left)\n",
    "        hits_l = len(pitcher_left[pitcher_left['events'] == 1])\n",
    "\n",
    "        if pas_l > 0:\n",
    "            batter['H/PA_against_L'] = hits_l/pas_l\n",
    "        else:\n",
    "            batter['H/PA_against_L'] = 0\n",
    "\n",
    "        if pas_r > 0 or pas_l > 0:\n",
    "            batter['H/PA_batter'] = (hits_r+hits_l)/(pas_r+pas_l)\n",
    "        else:\n",
    "            batter['H/PA_batter'] = 0\n",
    "        batter['avg_launch_angle'] = history['launch_angle'].mean()\n",
    "        batter['avg_launch_speed'] = history['launch_speed'].mean()\n",
    "        batter['xBA'] = history['estimated_ba_using_speedangle'].mean()\n",
    "\n",
    "        return(batter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59da52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepreprocess_historic_data(df):\n",
    "\n",
    "    pa_ending_events = np.array(['field_out',\n",
    "                        'strikeout',\n",
    "                        'single',\n",
    "                        'walk',\n",
    "                        'double',\n",
    "                        'home_run',\n",
    "                        'force_out',\n",
    "                        'grounded_into_double_play',\n",
    "                        'hit_by_pitch',\n",
    "                        'field_error',\n",
    "                        'sac_fly',\n",
    "                        'triple',\n",
    "                        'sac_bunt',\n",
    "                        'fielders_choice',\n",
    "                        'double_play',\n",
    "                        'fielders_choice_out',\n",
    "                        'strikeout_double_play',\n",
    "                        'catcher_interf',\n",
    "                        'sac_fly_double_play',\n",
    "                        'triple_play',\n",
    "                        'sac_bunt_double_play'])\n",
    "\n",
    "    hit_events = np.array(['single',\n",
    "                            'double',\n",
    "                            'home_run',\n",
    "                            'triple'])\n",
    "    \n",
    "    american_league_teams = np.array(['Boston Red Sox', 'Houston Astros', 'Chicago White Sox', 'Tampa Bay Rays', 'Oakland Athletics', 'Seattle Mariners', 'New York Yankees', 'Toronto Blue Jays', 'Los Angeles Angels', 'Cleveland Indians', 'Detroit Tigers', 'Kansas City Royals', 'Minnesota Twins', 'Texas Rangers', 'Baltimore Orioles'])\n",
    "    \n",
    "    df_filtered = df[df['events'].isin([1,0])]\n",
    "\n",
    "\n",
    "    venue_coords = pd.read_csv('Parks.csv')\n",
    "    \n",
    "    game_pk_df = pd.DataFrame(columns = statsapi.schedule(game_id=566083)[0].keys())\n",
    "\n",
    "    venue_dict = {}\n",
    "\n",
    "    for index, row in df_filtered.iterrows():\n",
    "        game_data = statsapi.schedule(game_id = row['game_pk'])[-1]\n",
    "        venue = game_data['venue_name']\n",
    "        game_pk_df = game_pk_df.append(game_data, ignore_index=True)\n",
    "\n",
    "            \n",
    "        try:\n",
    "            city = venue_coords[venue_coords['NAME'] == venue]['CITY'].iloc[0]\n",
    "            lat = venue_coords[venue_coords['NAME'] == venue]['Latitude'].iloc[0]\n",
    "            lon = venue_coords[venue_coords['NAME'] == venue]['Longitude'].iloc[0]\n",
    "            alt = venue_coords[venue_coords['NAME'] == venue]['Altitude'].iloc[0]\n",
    "            venue_dict[venue] = np.array((city, lat, lon, alt))\n",
    "        except IndexError:\n",
    "            raise IndexError(venue, index)\n",
    "\n",
    "    game_pk_df = game_pk_df.sort_values(['game_datetime'])\n",
    "\n",
    "    \n",
    "\n",
    "    game_pk_df['local_datetime'] = game_pk_df.apply(lambda row: convert_UTC_to_local(row), axis=1)\n",
    "    game_pk_df['local_datetime'] = game_pk_df['local_datetime'].apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "    \n",
    "    \n",
    "    datetime_coordinate_matching = {}\n",
    "    for index, row in game_pk_df.iterrows():\n",
    "        city, lat, lon, alt = venue_dict[row['venue_name']]\n",
    "        dc_datetime = row['local_datetime']\n",
    "        if dc_datetime in datetime_coordinate_matching.keys():\n",
    "            datetime_coordinate_matching[dc_datetime].append(','.join([str(lat), str(lon)]))\n",
    "        else:\n",
    "            datetime_coordinate_matching[dc_datetime] = [','.join([str(lat), str(lon)])]\n",
    "\n",
    "    game_pk_df['coordinates'] = game_pk_df['venue_name'].apply(lambda x: ','.join(venue_dict[x][1:3]))\n",
    "    game_pk_df['alt'] = game_pk_df['venue_name'].apply(lambda x: venue_dict[x][3])\n",
    "\n",
    "    from api_keys import visual_crossing\n",
    "    import requests\n",
    "\n",
    "    weather_df = pd.DataFrame()\n",
    "    for key, value in datetime_coordinate_matching.items():\n",
    "        url_locations = '|'.join(value)\n",
    "        URL = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?&aggregateHours=1&startDateTime={key}&endDateTime={key}&unitGroup=us&contentType=json&location={url_locations}&key={visual_crossing}'\n",
    "        global api_counter\n",
    "        if api_counter == 100000000:\n",
    "            print('Prepreprocessing will continue at ' + (datetime.now()+timedelta(seconds = 86400)).strftime('%H:%M:%S'))\n",
    "            sleep(86400)\n",
    "            api_counter = 0\n",
    "        try:\n",
    "            api_counter += 1\n",
    "            response = requests.get(URL)\n",
    "            data = response.json()\n",
    "            locations = list(data['locations'].keys())\n",
    "            for each in locations:\n",
    "                values = data['locations'][each]['values'][0]\n",
    "                weather_df = weather_df.append({**{'coordinates': each}, **values}, ignore_index=True)\n",
    "        except:\n",
    "            raise IndexError(key, value, response)\n",
    "\n",
    "    weather_df['datetimeStr'] = weather_df['datetimeStr'].apply(lambda x: x[:-6])\n",
    "    weather_df = weather_df.rename(columns={'datetimeStr': 'local_datetime'})\n",
    "\n",
    "    games_and_weather = pd.merge(\n",
    "        game_pk_df,\n",
    "        weather_df,\n",
    "        how=\"left\",\n",
    "        on=None,\n",
    "        left_on=['local_datetime', 'coordinates'],\n",
    "        right_on=['local_datetime', 'coordinates'],\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        sort=False,\n",
    "        suffixes=(\"_gpk\", \"_acw\"),\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "\n",
    "    df_detailed = pd.merge(\n",
    "        games_and_weather,\n",
    "        df_filtered,\n",
    "        how=\"right\",\n",
    "        on=None,\n",
    "        left_on='game_id',\n",
    "        right_on='game_pk',\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        sort=False,\n",
    "        suffixes=(\"_gaw\", \"_dff\"),\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "\n",
    "    print(df_detailed.info())\n",
    "\n",
    "\n",
    "    df_detailed['covid_doubleheader'] = df_detailed.apply(lambda row: 1 if row['game_year'] in [2020, 2021] and row['doubleheader'] =='Y' else 0, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    df_detailed['designated_hitter'] = df_detailed.apply(lambda row: 1 if row['home_name'] in american_league_teams or row['game_year'] == 2020 else 0, axis = 1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    pitcher_df = pd.concat([df_detailed[['game_id', 'game_date_gaw', 'away_probable_pitcher']].rename(columns={'away_probable_pitcher': 'probable_pitcher'}), df_detailed[['game_id', 'game_date_gaw', 'home_probable_pitcher']].rename(columns={'home_probable_pitcher': 'probable_pitcher'})])\n",
    "    pitcher_df = pitcher_df.drop_duplicates()\n",
    "    pitcher_df['probable_pitcher'] = pitcher_df['probable_pitcher'].replace('', np.nan)\n",
    "    pitcher_df = pitcher_df.dropna(subset=['probable_pitcher'])\n",
    "    pitcher_df = pitcher_df.reset_index(drop=True)\n",
    "\n",
    "    pitcher_stats = pitcher_df.apply(lambda row: pd.Series(derived_cumulative_stats_v2(row['probable_pitcher'], 'pitcher', end_date=row['game_date_gaw']-timedelta(days=1))), axis = 1)\n",
    "\n",
    "    pitcher_df = pd.merge(\n",
    "        pitcher_stats,\n",
    "        pitcher_df,\n",
    "        how=\"left\",\n",
    "        on=None,\n",
    "        left_on=None,\n",
    "        right_on=None,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        sort=False,\n",
    "        suffixes=(\"_s\", \"_df\"),\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "\n",
    "    home_batters = df_detailed[df_detailed['bottom'] == 1]\n",
    "    away_batters = df_detailed[df_detailed['bottom'] == 0]\n",
    "\n",
    "    df_wpitching_p1 = pd.merge(\n",
    "        pitcher_df,\n",
    "        home_batters,\n",
    "        how=\"right\",\n",
    "        on=None,\n",
    "        left_on=['game_id', 'probable_pitcher'],\n",
    "        right_on=['game_id', 'away_probable_pitcher'],\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        sort=False,\n",
    "        suffixes=(\"_p\", \"_df\"),\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "\n",
    "    df_wpitching_p2 = pd.merge(\n",
    "        pitcher_df,\n",
    "        away_batters,\n",
    "        how=\"right\",\n",
    "        on=None,\n",
    "        left_on=['game_id', 'probable_pitcher'],\n",
    "        right_on=['game_id', 'home_probable_pitcher'],\n",
    "        left_index=False,\n",
    "        right_index=False,\n",
    "        sort=False,\n",
    "        suffixes=(\"_p\", \"_df\"),\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "    df_wpitching = pd.concat([df_wpitching_p1, df_wpitching_p2], ignore_index=True)\n",
    "    df_wpitching = df_wpitching.sort_values(['game_date_gaw_df', 'game_pk','home_team', 'inning', 'bottom', 'outs_when_up', 'at_bat_number', 'pitch_number']).reset_index(drop=True)\n",
    "\n",
    "    df_wpitching['got_a_hit'] = df_wpitching.apply(lambda row: 1 if row['events'] == 1 else 0, axis=1)\n",
    "    df_wpitching = df_wpitching.sort_values('got_a_hit').drop_duplicates(subset=['game_id', 'batter'], keep='last').sort_values(['game_date_gaw_df', 'game_pk','home_team', 'inning', 'bottom', 'outs_when_up', 'at_bat_number', 'pitch_number']).reset_index(drop=True)\n",
    "\n",
    "    batting_stats = df_wpitching.apply(lambda row: pd.Series(derived_cumulative_stats_v2(row['batter'],'batter', end_date=row['game_date_gaw']-timedelta(days=1))), axis = 1)\n",
    "\n",
    "    df_final = pd.merge(\n",
    "        batting_stats,\n",
    "        df_wpitching,\n",
    "        how=\"right\",\n",
    "        on=None,\n",
    "        left_on=None,\n",
    "        right_on=None,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        sort=False,\n",
    "        suffixes=(\"_bs\", \"_df\"),\n",
    "        copy=True,\n",
    "        indicator=False,\n",
    "        validate=None,\n",
    "    )\n",
    "\n",
    "    df_final = df_final.rename(columns={'games_played_last_2_years_df': 'games_played_last_2_years_pitcher', 'games_played_last_2_years_bs': 'games_played_last_2_years_batter', 'bottom': 'home'})\n",
    "\n",
    "    df_final['local_datetime'] = pd.to_datetime(df_final['local_datetime'])\n",
    "    df_final['local_date'] = df_final['local_datetime'].apply(lambda x: x.toordinal())\n",
    "    df_final['local_datetime'] = df_final['local_datetime'].apply(lambda x: x.timestamp())\n",
    "\n",
    "\n",
    "    df_final =df_final.drop(columns= ['player_name',\n",
    "     'game_type_dff', 'pitch_type',\n",
    "     'release_speed',\n",
    "     'release_pos_x',\n",
    "     'release_pos_z',\n",
    "     'pitcher',\n",
    "     'events',\n",
    "     'description',\n",
    "     'spin_dir',\n",
    "    'spin_rate_deprecated',\n",
    "     'break_angle_deprecated',\n",
    "     'break_length_deprecated',\n",
    "     'zone',\n",
    "     'des',\n",
    "      'p_throws',\n",
    "    'type',\n",
    "     'hit_location',\n",
    "     'bb_type',\n",
    "     'balls',\n",
    "     'strikes',\n",
    "     'pfx_x',\n",
    "     'pfx_z',\n",
    "     'plate_x',\n",
    "     'plate_z',\n",
    "     'on_3b',\n",
    "     'on_2b',\n",
    "     'on_1b',\n",
    "     'outs_when_up',\n",
    "     'inning',\n",
    "     'hc_x',\n",
    "     'hc_y',\n",
    "     'tfs_deprecated',\n",
    "     'tfs_zulu_deprecated',\n",
    "      'sv_id',\n",
    "     'vx0',\n",
    "     'vy0',\n",
    "     'vz0',\n",
    "     'ax',\n",
    "     'ay',\n",
    "     'az',\n",
    "     'sz_top',\n",
    "     'sz_bot',\n",
    "     'hit_distance_sc',\n",
    "     'launch_speed',\n",
    "     'launch_angle',\n",
    "     'effective_speed',\n",
    "     'release_spin_rate',\n",
    "     'release_extension',\n",
    "     'pitcher.1',\n",
    "     'fielder_2.1',\n",
    "     'fielder_3',\n",
    "     'fielder_4',\n",
    "     'fielder_5',\n",
    "     'fielder_6',\n",
    "     'fielder_7',\n",
    "     'fielder_8',\n",
    "     'fielder_9',\n",
    "     'release_pos_y',\n",
    "     'estimated_ba_using_speedangle',\n",
    "     'estimated_woba_using_speedangle',\n",
    "     'woba_value',\n",
    "     'woba_denom',\n",
    "     'babip_value',\n",
    "     'iso_value',\n",
    "     'launch_speed_angle',\n",
    "     'at_bat_number',\n",
    "     'pitch_number',\n",
    "     'pitch_name',\n",
    "     'home_score_dff',\n",
    "     'away_score_dff',\n",
    "     'bat_score',\n",
    "     'fld_score',\n",
    "     'post_away_score',\n",
    "     'post_home_score',\n",
    "     'post_bat_score',\n",
    "     'post_fld_score',\n",
    "     'if_fielding_alignment',\n",
    "     'of_fielding_alignment',\n",
    "     'spin_axis',\n",
    "     'delta_home_win_exp',\n",
    "     'delta_run_exp',\n",
    "     'game_pk',\n",
    "     'game_id','game_date_gaw_p',\n",
    "     'game_datetime',\n",
    "     'game_date_gaw_df',\n",
    "     'game_type_gaw',\n",
    "     'datetime',\n",
    "    #  'datetimeStr',\n",
    "     'game_date_dff','winning_team',\n",
    "     'losing_team',\n",
    "     'winning_pitcher',\n",
    "     'losing_pitcher',\n",
    "     'save_pitcher',\n",
    "     'summary', 'home_probable_pitcher',\n",
    "     'away_probable_pitcher',\n",
    "     'home_pitcher_note',\n",
    "     'away_pitcher_note',\n",
    "     'away_score_gaw',\n",
    "     'home_score_gaw',\n",
    "     'current_inning',\n",
    "     'inning_state',\n",
    "     'venue_id',\n",
    "     'status',\n",
    "     'home_team', 'away_team', 'home_id', 'away_id'])\n",
    "    \n",
    "    for col in df_final.columns.to_list():\n",
    "        if len(df_final[col].unique()) == 1:\n",
    "            df_final = df_final.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8aa2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in [2017, 2018, 2019, 2020, 2021]:\n",
    "    statcast_importer(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "sample = prepreprocess_historic_data(pd.concat([\n",
    "                pd.read_csv('2019_statcast_pbp.csv'),\n",
    "                pd.read_csv('2020_statcast_pbp.csv'),\n",
    "               pd.read_csv('2021_statcast_pbp.csv')\n",
    "                ], ignore_index=True))\n",
    "print(datetime.now() - start)\n",
    "start = datetime.now()\n",
    "holdout = prepreprocess_historic_data(pybaseball.statcast('2021-07-01', (datetime.now()-timedelta(weeks=1)).strftime('%Y-%m-%d')))\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_mlb():\n",
    "    \"\"\"Get list of MLB team IDs for the purposes of pulling active rosters\"\"\"\n",
    "    params = {\n",
    "        \"activeStatus\": 'Y',\n",
    "        \"season\": 2021,\n",
    "        \"sportIds\": 1,\n",
    "        \"fields\": \"teams,id,name,league\",\n",
    "    }\n",
    "    \n",
    "    r= statsapi.get('teams', params)\n",
    "    \n",
    "    teams = {}\n",
    "    for team in r['teams']:\n",
    "        if team['league']['id'] in [103, 104]:\n",
    "            teams[team['name']] = team['id']\n",
    "    \n",
    "    return (teams)\n",
    "\n",
    "def roster(teamId, rosterType=None, season=datetime.now().year, date=None):\n",
    "    \"\"\"Get the roster for a given team.\"\"\"\n",
    "    if not rosterType:\n",
    "        rosterType = \"active\"\n",
    "\n",
    "    params = {\"rosterType\": rosterType, \"season\": season, \"teamId\": teamId}\n",
    "    if date:\n",
    "        params.update({\"date\": date})\n",
    "\n",
    "    r = statsapi.get(\"team_roster\", params)\n",
    "\n",
    "    players = []\n",
    "    for x in r[\"roster\"]:\n",
    "        players.append(\n",
    "            x[\"person\"][\"id\"]\n",
    "        )\n",
    "\n",
    "    \n",
    "    return (players)\n",
    "\n",
    "def lookup_team(lookup_value, activeStatus=\"Y\", season=datetime.now().year, sportIds=1):\n",
    "    \"\"\"Get a info about a team or teams based on the team name, city, abbreviation, or file code.\"\"\"\n",
    "    params = {\n",
    "        \"activeStatus\": activeStatus,\n",
    "        \"season\": season,\n",
    "        \"sportIds\": sportIds,\n",
    "        \"fields\": \"teams,id,name,teamCode,fileCode,teamName,locationName,shortName,league\",\n",
    "    }\n",
    "    r = statsapi.get(\"teams\", params)\n",
    "\n",
    "    teams = []\n",
    "    for team in r[\"teams\"]:\n",
    "        for v in team.values():\n",
    "            if str(lookup_value).lower() in str(v).lower():\n",
    "                teams.append(team)\n",
    "                break\n",
    "\n",
    "    return teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e038aa42",
   "metadata": {},
   "source": [
    "# MODEL WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(estimator, X_train, X_val, y_train, y_val, holdout, roc_auc='proba', output = False):\n",
    "\n",
    "    X_holdout = holdout.drop(['got_a_hit'], axis=1)\n",
    "    y_holdout = holdout['got_a_hit']\n",
    "    \n",
    "    #     grab predictions\n",
    "    train_preds = estimator.predict(X_train)\n",
    "    val_preds = estimator.predict(X_val)\n",
    "    holdout_preds = estimator.predict(X_holdout)\n",
    "    \n",
    "#     output needed for roc_auc score\n",
    "    if roc_auc == 'skip':\n",
    "        train_out = False\n",
    "        val_out = False\n",
    "        holdout_out = False\n",
    "    elif roc_auc == 'dec': # not all classifiers have a decision function\n",
    "        train_out = estimator.decision_function(X_train)\n",
    "        val_out = estimator.decision_function(X_val)\n",
    "        holdout_out = estimator.decision_function(X_holdout)\n",
    "    elif roc_auc == 'proba':\n",
    "        try:\n",
    "            train_out = estimator.predict_proba(X_train)[:, 1]\n",
    "            val_out = estimator.predict_proba(X_val)[:, 1]\n",
    "            holdout_out = estimator.predict_proba(X_holdout)[:, 1]\n",
    "        except AttributeError:\n",
    "            train_out = estimator.predict(X_train)\n",
    "            val_out = estimator.predict(X_val)\n",
    "            holdout_out = estimator.predict(X_holdout)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"The value for roc_auc should be 'skip', 'dec', or 'proba'.\")\n",
    "    \n",
    "    ac = accuracy_score(y_train, train_preds)\n",
    "    f1 = f1_score(y_train, train_preds)\n",
    "    ras = roc_auc_score(y_train, train_out)\n",
    "    pr = precision_score(y_train, train_preds)\n",
    "    \n",
    "    if output == True:\n",
    "        print('Train Scores')\n",
    "        print('------------')\n",
    "        print(f'Accuracy: {ac}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        if type(train_out) == np.ndarray:\n",
    "            print(f'ROC-AUC: {ras}')\n",
    "        print(f'Precision: {pr}')\n",
    "\n",
    "    ac = accuracy_score(y_val, val_preds)\n",
    "    f1 = f1_score(y_val, val_preds)\n",
    "#     print(type(y_val))\n",
    "    ras = roc_auc_score(y_val, val_out)\n",
    "    pr = precision_score(y_val, val_preds)\n",
    "    \n",
    "    if output == True:\n",
    "        print('-----------------------------------')\n",
    "        print('Val Scores')\n",
    "        print('-----------')\n",
    "        print(f'Accuracy: {ac}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        if type(val_out) == np.ndarray:\n",
    "            print(f'ROC-AUC: {ras}')\n",
    "        print(f'Precision: {pr}')\n",
    "    \n",
    "    ac = accuracy_score(y_holdout, holdout_preds)\n",
    "    f1 = f1_score(y_holdout, holdout_preds)\n",
    "#     print(type(holdout['got_a_hit']))\n",
    "    ras = roc_auc_score(y_holdout, holdout_out)\n",
    "    pr = precision_score(y_holdout, holdout_preds)\n",
    "    \n",
    "    if output == True:\n",
    "        print('-----------------------------------')\n",
    "        print('Holdout Scores')\n",
    "        print('-----------')\n",
    "        print(f'Accuracy: {ac}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        if type(holdout_out) == np.ndarray:\n",
    "            print(f'ROC-AUC: {ras}')\n",
    "        print(f'Precision: {pr}')\n",
    "\n",
    "        print('\\nVal Data')\n",
    "        print('-----------')\n",
    "\n",
    "        plot_confusion_matrix(estimator, X_val, y_val, values_format=',.5g')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Holdout Data')        \n",
    "        print('-----------')\n",
    "\n",
    "        plot_confusion_matrix(estimator, X_holdout, y_holdout, values_format=',.5g')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streak_checker(dataframe, output = False, metric = 'odds', ascending=False, combiner=None, timed=False):\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        if 'L/R_split' in metric and combiner == 'add':\n",
    "            if timed == True:\n",
    "                start = datetime.now()\n",
    "\n",
    "                print(combiner, start)\n",
    "            dataframe_copy = dataframe.copy()\n",
    "            dataframe_copy['temp'] = dataframe.apply(lambda row: row['H/PA_against_R'] + row['against_lefties_H/PA'] if (row['pitch_hand'] == 'Right' and row['stand'] == 'L') else (row['H/PA_against_L'] + row['against_righties_H/PA'] if (row['pitch_hand'] == 'Left' and row['stand'] == 'R') else 0), axis=1)\n",
    "            mms = MinMaxScaler()\n",
    "            dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "            \n",
    "            if timed == True:\n",
    "                print(datetime.now() - start)\n",
    "            dataframe = dataframe_copy\n",
    "            metric='temp'\n",
    "        elif 'L/R_split' in metric and combiner == 'multiply':\n",
    "            if timed == True:\n",
    "                start = datetime.now()\n",
    "\n",
    "                print(combiner, start)\n",
    "            dataframe_copy = dataframe.copy()\n",
    "            dataframe_copy['temp'] = dataframe.apply(lambda row: row['H/PA_against_R'] * row['against_lefties_H/PA'] if (row['pitch_hand'] == 'Right' and row['stand'] == 'L') else (row['H/PA_against_L'] * row['against_righties_H/PA'] if (row['pitch_hand'] == 'Left' and row['stand'] == 'R') else 0), axis=1)\n",
    "            mms = MinMaxScaler()\n",
    "            dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "            \n",
    "            if timed == True:\n",
    "                print(datetime.now() - start)\n",
    "            dataframe = dataframe_copy\n",
    "            metric='temp'\n",
    "        if type(metric) == list:\n",
    "            dataframe_copy = dataframe.copy()\n",
    "            if combiner == 'add':\n",
    "                if timed == True:\n",
    "                    start = datetime.now()\n",
    "\n",
    "                    print(combiner, start)\n",
    "                dataframe_copy['temp'] = dataframe.apply(lambda row: row[metric[0]] + row[metric[1]], axis=1)\n",
    "                mms = MinMaxScaler()\n",
    "                dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "                \n",
    "                if timed == True:\n",
    "                    print(datetime.now() - start)\n",
    "            elif combiner == 'multiply':\n",
    "                if timed == True:\n",
    "                    start = datetime.now()\n",
    "\n",
    "                    print(combiner, start)\n",
    "                dataframe_copy['temp'] = dataframe.apply(lambda row: row[metric[0]] * row[metric[1]], axis=1)\n",
    "                mms = MinMaxScaler()\n",
    "                dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "                \n",
    "                if timed == True:\n",
    "                    print(datetime.now() - start)\n",
    "            \n",
    "            else:\n",
    "                raise KeyError(f'{combiner} combiners have not been implemented yet')\n",
    "            dataframe = dataframe_copy\n",
    "            metric = 'temp'\n",
    "        else:\n",
    "            pass\n",
    "    except KeyError:\n",
    "        raise KeyError(\"metric must be chosen from dataframe's columns\")\n",
    "    \n",
    "    date_list = np.sort(dataframe['local_date'].unique())\n",
    "    \n",
    "    for day in date_list:\n",
    "        best_bet = dataframe[dataframe['local_date'] == day].sort_values(metric, ascending=ascending).iloc[0]\n",
    "        \n",
    "        odds = best_bet[metric]\n",
    "        date = datetime.fromordinal(best_bet['local_date']).strftime(\"%b %d %Y\")\n",
    "        \n",
    "        player_info = pybaseball.playerid_reverse_lookup([best_bet['batter']])\n",
    "        if len(player_info) >1:\n",
    "            player_info = player_info.iloc[0]\n",
    "            player = player_info['key_mlbam']\n",
    "        elif player_info.empty:\n",
    "            player_info = playerid_lookup(player[0], player[1], fuzzy=True).iloc[0]\n",
    "            player = player_info['key_mlbam']\n",
    "        else:\n",
    "            try:\n",
    "                player = ' '.join(player_info.iloc[0][['name_first', 'name_last']])\n",
    "            except:\n",
    "                raise NameError(player_info, player)\n",
    "        if best_bet['home'] == 0:\n",
    "            team = best_bet['away_name']\n",
    "        else:\n",
    "            team = best_bet['home_name']\n",
    "    #     team = statsapi.lookup_team(player['currentTeam']['id'])[0]['name']\n",
    "    #     player_name = player['fullName']\n",
    "        actual_result = best_bet['got_a_hit']\n",
    "        results.append(actual_result)\n",
    "        if output == True:\n",
    "            if day > date_list[-20]:\n",
    "                pass\n",
    "#                 print(date, player, odds, actual_result)\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for num in results:\n",
    "        if num == 1:\n",
    "            current += 1\n",
    "        else:\n",
    "            longest = max(longest, current)\n",
    "            current = 0\n",
    "    if output == True:\n",
    "        print(f'Longest streak in set ({len(results)} days): ', longest)\n",
    "        print(f'Total correct guesses in set ({len(results)} days): ', sum(results), sum(results)/len(results))\n",
    "#     return (dataframe, best_bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11793f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(clf, X_train, X_val, y_train, y_val, holdout):\n",
    "    probability = True\n",
    "    \n",
    "    clf.fit(X_train.head(100), y_train.head(100))\n",
    "\n",
    "    evaluate(clf, X_train.head(100), X_val.head(100), y_train.head(100), y_val.head(100), holdout);\n",
    "\n",
    "    runtime = datetime.now() - start\n",
    "\n",
    "    if runtime > timedelta(seconds=5):\n",
    "        print(f'Model would take {runtime.total_seconds()*200/60} minutes to run.')\n",
    "    else:\n",
    "        print('Model should take between {:.0f} and {:.0f} minutes to run and finish by {}'.format(runtime.total_seconds()*20/60, runtime.total_seconds()*200/60, (datetime.now()+(timedelta(seconds=(runtime.total_seconds()*200)))).strftime(\"%H:%M\")))\n",
    "        clf.fit(X_train, y_train)\n",
    "        evaluate(clf, X_train, X_val, y_train, y_val, holdout, output=True)\n",
    "        plot_roc_curve(clf, X_val, y_val)\n",
    "        plt.show()\n",
    "        \n",
    "        try:\n",
    "            val_df_odds = pd.Series(clf.predict_proba(X_val)[:, 1], name='odds')\n",
    "            holdout_df_odds = pd.Series(clf.predict_proba(holdout.drop(['got_a_hit'], axis=1))[:, 1], name='odds')\n",
    "        except AttributeError:\n",
    "            probability = False\n",
    "            val_df_odds = pd.Series(clf.predict(X_val), name= 'odds')\n",
    "            holdout_df_odds = pd.Series(clf.predict(holdout.drop(['got_a_hit'], axis=1)), name='odds')\n",
    "\n",
    "        val_df = X_val.assign(got_a_hit = y_val).reset_index(drop=True)\n",
    "        val_df = val_df.assign(odds=val_df_odds)\n",
    "        holdout = holdout.reset_index(drop=True)\n",
    "        holdout = holdout.assign(odds=val_df_odds)\n",
    "        if set(val_df_odds.value_counts().index) == set([1, 0]):\n",
    "            print(str(clf.get_params()['steps'][1][1]), \"doesn't return probabilities, so no streak results will be returned based on this alone.\")\n",
    "            pass\n",
    "        else:\n",
    "            print('-----------------------------------')\n",
    "\n",
    "            print('Val data')\n",
    "            print('-----------')\n",
    "\n",
    "            streak_checker(val_df, output=True)\n",
    "            print('Holdout data')\n",
    "            print('-----------')\n",
    "\n",
    "            streak_checker(holdout, output=True)\n",
    "            \n",
    "        print('Odds multiplied by PA/G_batter')\n",
    "        print('-----------')\n",
    "        \n",
    "        print('Val data')\n",
    "        print('-----------')\n",
    "        streak_checker(val_df, output=True, metric=['PA/G_batter', 'odds'], combiner='multiply')\n",
    "        print('Holdout data')\n",
    "        print('-----------')\n",
    "        streak_checker(holdout, output=True, metric=['PA/G_batter', 'odds'], combiner='multiply')\n",
    "        \n",
    "        print('Odds added to PA/G_batter')\n",
    "        print('-----------')\n",
    "\n",
    "        print('Val data')\n",
    "        print('-----------')\n",
    "        streak_checker(val_df, output=True, metric=['L/R_split', 'odds'], combiner='add')\n",
    "        print('Holdout data')\n",
    "        print('-----------')\n",
    "        streak_checker(holdout, output=True, metric=['L/R_split', 'odds'], combiner='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed50044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=.25):\n",
    "    \n",
    "    '''My simplified train_test_split which just cuts the dataframe by date, in order to accurately evaluate streak results, to roughly the test size requested'''\n",
    "    \n",
    "    sample_size = len(X)\n",
    "    \n",
    "    train_size = 1 - test_size\n",
    "    \n",
    "    train_size = round(train_size * sample_size)\n",
    "    \n",
    "    df = df.sort_values(['local_date']).reset_index(drop=True)\n",
    "    \n",
    "    date_cutoff = df.iloc[train_size]['local_date']\n",
    "\n",
    "    train = df[df['local_date'] <= date_cutoff]\n",
    "    val = df[df['local_date'] > date_cutoff]\n",
    "    \n",
    "    X_train = train.drop(['got_a_hit'], axis=1)\n",
    "    y_train = train['got_a_hit']\n",
    "    \n",
    "    X_val = val.drop(['got_a_hit'], axis=1)\n",
    "    y_val = val['got_a_hit']\n",
    "    \n",
    "    return(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [] # all numeric columns\n",
    "cols_to_ohe = [] # doubleheader, conditions, stand\n",
    "cols_to_targ = [] # all other object columns\n",
    "\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype in ['float64', 'int64'] and c not in ['game_num', 'batter', 'fielder_2', 'umpire']:\n",
    "        num_cols.append(c)\n",
    "    elif len(X_train[c].unique()) < 10:\n",
    "        cols_to_ohe.append(c)\n",
    "    else:\n",
    "        cols_to_targ.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "ohe_transformer = Pipeline(steps=[\n",
    "    ('ohe_imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohencoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "target_encoder = Pipeline(steps=[\n",
    "    ('freq_enc', ce.target_encoder.TargetEncoder()),\n",
    "    ('freq_imputer', SimpleImputer(strategy='constant', fill_value=0))\n",
    "    \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('ohe', ohe_transformer, cols_to_ohe),\n",
    "        ('target', target_encoder, cols_to_targ)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769e1f6",
   "metadata": {},
   "source": [
    "# Baseline - dummy models\n",
    "Pick player with the highest Plate Appearances per Game over the last 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcadde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(df, output=True, metric='PA/G_batter')\n",
    "streak_checker(test_df, output=True, metric='PA/G_batter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969b4d7",
   "metadata": {},
   "source": [
    "Observations - this seems to trend towards a small group of players except for when a player makes their mLB debut towards the top of the lineup (Wander Franco, Greg Deichmann). Sample size is an issue - could be addressed by either looking at games played or excluding PA/G above 4.9.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126d722",
   "metadata": {},
   "source": [
    "Pick player with highest launch speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ef4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(df, output=True, metric='avg_launch_speed')\n",
    "streak_checker(test_df, output=True, metric='avg_launch_speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e28d94",
   "metadata": {},
   "source": [
    "The metrics point to this being mostly useless which is backed up by the fact that it seems to occasionally choose pitchers (Hunter Strickland, Zack Greinke), when it's not fawning over Aaron Judge. Could be worth excluding pitchers as a class though would be hard to implement with the current dataset, not to mention the fact that 2-way players would need to be excluded from that filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817da378",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(df, output=True, metric=['avg_launch_angle', 'avg_launch_speed'], combiner='add')\n",
    "streak_checker(test_df, output=True, metric=['avg_launch_angle', 'avg_launch_speed'], combiner='add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122babd2",
   "metadata": {},
   "source": [
    "Exaserbates bias towards small sample sizes, leading part time players to dominate selection rather than proven hitters\n",
    "\n",
    "Would also be worth checking lefty righty splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(df, output=True, metric='L/R_split', combiner='add')\n",
    "streak_checker(test_df, output=True, metric='L/R_split', combiner='add')\n",
    "streak_checker(df, output=True, metric='L/R_split', combiner='multiply')\n",
    "streak_checker(test_df, output=True, metric='L/R_split', combiner='multiply')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a6c981",
   "metadata": {},
   "source": [
    "Nothing stands out in these groups. Seems to be a mix of journeymen and all-stars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7b137",
   "metadata": {},
   "source": [
    "# Model 1 - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('tree', dt)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d575355",
   "metadata": {},
   "source": [
    "# Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "logreg = LogisticRegression(solver='sag')\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logreg', logreg)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cd098",
   "metadata": {},
   "source": [
    "# Model 3: SVC too slow, linearSVC doesn't supply probability, so I need to supply a secondary way to select a single batter per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "Linearsvc = LinearSVC()\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('linearsvc', Linearsvc)\n",
    "])\n",
    "\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7490cc4",
   "metadata": {},
   "source": [
    "# Model 4: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf381b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', knn)\n",
    "])\n",
    "\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2f36f",
   "metadata": {},
   "source": [
    "# Model 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e48e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rfc', rfc)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7922e",
   "metadata": {},
   "source": [
    "# Model 6: AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ada', ada)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7ad36",
   "metadata": {},
   "source": [
    "# Model 7: GradientBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a54514",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('gbm', gbm)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c462d15",
   "metadata": {},
   "source": [
    "# Model 8: XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f33776",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "xbg = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xbg', xbg)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c83e7",
   "metadata": {},
   "source": [
    "# Model 9: Random Forest with Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "parameters = {\n",
    "    'min_samples_split': [3, 5, 100], \n",
    "    'n_estimators' : [100, 300],\n",
    "        'max_depth': [3, 5, 15, 25],\n",
    "\n",
    "}\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2e0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133d8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0383bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be run daily\n",
    "def daily_guess():\n",
    "\n",
    "    # get game data, then weather data, then pitcher data, then batter data\n",
    "\n",
    "    todays_games = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for game in statsapi.schedule():\n",
    "    #     grab 'away_probable_pitcher','home_probable_pitcher','away_name',\n",
    "    #     'home_name','doubleheader','game_num','venue_name','game_datetime'\n",
    "\n",
    "    #     df['local_datetime'] = pd.to_datetime(df['local_datetime'])\n",
    "    #     df['local_date'] = df['local_datetime'].apply(lambda x: x.toordinal())\n",
    "    #     df['local_datetime'] = df['local_datetime'].apply(lambda x: x.timestamp())\n",
    "\n",
    "        row = {\n",
    "            'game_id': game['game_id'],\n",
    "            'away_probable_pitcher': game['away_probable_pitcher'],\n",
    "            'home_probable_pitcher': game['home_probable_pitcher'],\n",
    "            'away_name': game['away_name'],\n",
    "            'home_name': game['home_name'],\n",
    "            'doubleheader': game['doubleheader'],\n",
    "            'game_num': game['game_num'],\n",
    "            'venue_name': game['venue_name'],\n",
    "            'game_datetime': game['game_datetime']\n",
    "        }\n",
    "\n",
    "        todays_games = todays_games.append(row, ignore_index=True)\n",
    "\n",
    "    todays_games = todays_games.replace('', np.nan)    \n",
    "\n",
    "    games_and_weather = time_date_weather_data(todays_games)\n",
    "\n",
    "    away_pitchers = games_and_weather['away_probable_pitcher']\n",
    "    home_pitchers = games_and_weather['home_probable_pitcher']\n",
    "    pitchers_stats = pd.DataFrame()\n",
    "\n",
    "    for a in away_pitchers:\n",
    "        print(a)\n",
    "        if type(a) == float:\n",
    "            pitchers_stats = pitchers_stats.append(pd.Series(), ignore_index=True)\n",
    "        else:\n",
    "            pitchers_stats = pitchers_stats.append({**derived_cumulative_stats_v2(a, 'pitcher', end_date=(datetime.now()-timedelta(seconds=86400)).strftime(\"%Y-%m-%d\")), **{'probable_pitcher': a}}, ignore_index=True)\n",
    "\n",
    "    for h in home_pitchers:\n",
    "        print(h)\n",
    "        if type(h) == float:\n",
    "            pitchers_stats = pitchers_stats.append(pd.Series(), ignore_index=True)\n",
    "        else:\n",
    "            pitchers_stats = pitchers_stats.append({**derived_cumulative_stats_v2(h, 'pitcher', end_date=(datetime.now()-timedelta(seconds=86400)).strftime(\"%Y-%m-%d\")), **{'probable_pitcher': h}}, ignore_index=True)\n",
    "\n",
    "    batting_stats = pd.DataFrame()\n",
    "    for index, game in games_and_weather.iterrows():\n",
    "        print(game[['away_name', 'home_name']])\n",
    "    #     home batters, away pitcher\n",
    "        batting_team_id = lookup_mlb()[game['home_name']]\n",
    "        batting_roster = roster(batting_team_id)\n",
    "        for b in batting_roster:\n",
    "            start = datetime.now()\n",
    "            batting_stats = batting_stats.append({**derived_cumulative_stats_v2(b, 'batter', end_date=(datetime.now()-timedelta(seconds=86400)).strftime(\"%Y-%m-%d\")), **pitchers_stats[pitchers_stats['probable_pitcher'] == game['away_probable_pitcher']], **game[['away_name',\n",
    "                                             'home_name',\n",
    "                                             'doubleheader',\n",
    "                                             'game_num',\n",
    "                                             'venue_name',\n",
    "                                             'local_datetime',\n",
    "                                             'coordinates',\n",
    "                                             'cloudcover',\n",
    "                                             'conditions',\n",
    "                                             'dew',\n",
    "                                             'heatindex',\n",
    "                                             'humidity',\n",
    "                                             'maxt',\n",
    "                                             'mint',\n",
    "                                             'precip',\n",
    "                                             'sealevelpressure',\n",
    "                                             'snowdepth',\n",
    "                                             'temp',\n",
    "                                             'visibility',\n",
    "                                             'wdir',\n",
    "                                             'weathertype',\n",
    "                                             'wgust',\n",
    "                                             'windchill',\n",
    "                                             'wspd'\n",
    "     ]]}, ignore_index=True)\n",
    "            print(datetime.now()-start)\n",
    "        #     away batters, home pitcher\n",
    "        batting_team_id = lookup_mlb()[game['away_name']]\n",
    "        batting_roster = roster(batting_team_id)\n",
    "        for b in batting_roster:\n",
    "            batting_stats = batting_stats.append({**derived_cumulative_stats_v2(b, 'batter', end_date=(datetime.now()-timedelta(seconds=86400)).strftime(\"%Y-%m-%d\")), **pitchers_stats[pitchers_stats['probable_pitcher'] == game['home_probable_pitcher']], **game[['away_name',\n",
    "                                             'home_name',\n",
    "                                             'doubleheader',\n",
    "                                             'game_num',\n",
    "                                             'venue_name',\n",
    "                                             'local_datetime',\n",
    "                                             'coordinates',\n",
    "                                             'cloudcover',\n",
    "                                             'conditions',\n",
    "                                             'dew',\n",
    "                                             'heatindex',\n",
    "                                             'humidity',\n",
    "                                             'maxt',\n",
    "                                             'mint',\n",
    "                                             'precip',\n",
    "                                             'sealevelpressure',\n",
    "                                             'snowdepth',\n",
    "                                             'temp',\n",
    "                                             'visibility',\n",
    "                                             'wdir',\n",
    "                                             'weathertype',\n",
    "                                             'wgust',\n",
    "                                             'windchill',\n",
    "                                             'wspd'\n",
    "     ]]}, ignore_index=True)\n",
    "\n",
    "# TODO\n",
    "# RUN batting_stats through predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d822c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
