{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e674b187",
   "metadata": {},
   "source": [
    "# Modeling How to Beat the Streak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340034bf",
   "metadata": {},
   "source": [
    "In 1941, Joe DiMaggio recorded hits in 56 consecutive games of Major League Baseball. This record has stood for 80 years and is seen as one of the most unlikely to be broken. In 2001, MLB started a fantasy game called Beat the Streak, challenging fans to simply pick, among all players, one who would get a hit on a given day. And then do that for 57 consecutive days to symbolically beat DiMaggio's streak. In 20 years nobody has won but a few have come as close as 51.\n",
    "\n",
    "In this notebook I aim to take a statistical approach to determining these daily picks in order to give myself the best possible chance I can to win this competition and, with it, the $5.6 million prize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7fa2a6",
   "metadata": {},
   "source": [
    "My first step will be to gather the relevant data and assemble it in a useful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "import pybaseball\n",
    "import statsapi\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72689417",
   "metadata": {},
   "source": [
    "pybaseball.statcast allows me to gather details of every MLB game since 2017 down to the individual pitch level. As this alone is a lot of data I'll save it as I gather it so I don't have to rerun this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've set a limit for my data to between the beginning of the 2019 season and June 30th, 2021.. \n",
    "\n",
    "data = pybaseball.statcast('2019-03-20', '2019-09-29')\n",
    "\n",
    "data.to_csv('untouched_2019_statcast_pbp.csv')\n",
    "\n",
    "data = pybaseball.statcast('2020-07-23', '2020-09-27')\n",
    "\n",
    "data.to_csv('untouched_2020_statcast_pbp.csv')\n",
    "\n",
    "data = pybaseball.statcast('2021-04-01', '2021-06-30')\n",
    "\n",
    "data.to_csv('untouched_2021_statcast_pbp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('untouched_2019_statcast_pbp.csv', index_col=0),\n",
    "                pd.read_csv('untouched_2020_statcast_pbp.csv', index_col=0),\n",
    "               pd.read_csv('untouched_2021_statcast_pbp.csv', index_col=0)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a16b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991f726",
   "metadata": {},
   "source": [
    "The various features vary wildly in count. While some of this variation would be expected for columns like \"on_3b\", \"on_2b\" and \"on_1b,\" others demonstrate a change in how the data has been collected. For example, despite the presence of an umpire column it doesn't seem like statcast presently collects information about the people umpiring these games.\n",
    "\n",
    "To start my data cleaning I'll drop columns with only one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "        if len(df[col].unique()) == 1 and col not in ['home_']:\n",
    "            df = df.drop([col], axis=1)\n",
    "print('dropping columns done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95be60",
   "metadata": {},
   "source": [
    "Now to explore some individual columns to see if they need to be transformed in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pitch_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561e76a",
   "metadata": {},
   "source": [
    "I'll likely need to transform this into ordinals later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d831b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['batter'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybaseball.playerid_reverse_lookup([543760, 656803], key_type='mlbam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pitcher'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['game_pk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ded2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsapi.schedule(game_id=633588)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0033a6a",
   "metadata": {},
   "source": [
    "These three columns refer to an internal ID system which will have to be consulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['events'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ba7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf8f75",
   "metadata": {},
   "source": [
    "One thing that's become apparent to me at this point is that many of these features are not necessarily things the user would know prior to the game, such as how hard an individual ball is hit. These features will have to be dropped.\n",
    "\n",
    "Additionally, some information that the user would know that could be important, such as who the expected starting pitchers are, are not included. At this point we'll use game_pk to add in more information through the MLB-statsapi library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5976c",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30802551",
   "metadata": {},
   "source": [
    "\"game_datetime\" seems to be in UTC. In order to account for possible differences between day and night games I'll convert it to local time and then to a timestamp.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893d4fb",
   "metadata": {},
   "source": [
    "Now because of the size of the data and the way I'll be manipulating it later, I found it useful to change a number of the object columns into integers and explicitly set types to the smallest possible as early as possible in the process to cut down on run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d64d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_ending_events = np.array(['field_out',\n",
    "                                'strikeout',\n",
    "                                'single',\n",
    "                                'walk',\n",
    "                                'double',\n",
    "                                'home_run',\n",
    "                                'force_out',\n",
    "                                'grounded_into_double_play',\n",
    "                                'hit_by_pitch',\n",
    "                                'field_error',\n",
    "                                'sac_fly',\n",
    "                                'triple',\n",
    "                                'sac_bunt',\n",
    "                                'fielders_choice',\n",
    "                                'double_play',\n",
    "                                'fielders_choice_out',\n",
    "                                'strikeout_double_play',\n",
    "                                'catcher_interf',\n",
    "                                'sac_fly_double_play',\n",
    "                                'triple_play',\n",
    "                                'sac_bunt_double_play'])\n",
    "\n",
    "hit_events = np.array(['single',\n",
    "                        'double',\n",
    "                        'home_run',\n",
    "                        'triple'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_types(df):\n",
    "    \n",
    "#     First we need to fill in the  missing values in numeric columns to make the type transformation possible\n",
    "    \n",
    "    num_columns = ['release_speed', 'batter', 'pitcher', 'zone', 'hit_location', 'balls', 'strikes', 'game_year', 'on_3b', 'on_2b', 'on_1b', 'outs_when_up', 'inning', 'fielder_2', 'hit_distance_sc', 'release_spin_rate', 'game_pk', 'pitcher.1', 'fielder_2.1', 'fielder_3', 'fielder_4', 'fielder_5', 'fielder_6', 'fielder_7', 'fielder_8', 'fielder_9', 'woba_value', 'woba_denom', 'babip_value', 'at_bat_number', 'pitch_number', 'home_score', 'away_score', 'bat_score', 'fld_score', 'post_away_score', 'post_home_score', 'post_bat_score', 'post_fld_score']\n",
    "\n",
    "    num_transformer = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer(strategy='median'))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', num_transformer, num_columns)])\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    t = clf.fit_transform(df)\n",
    "    \n",
    "    df[num_columns] = t\n",
    "    \n",
    "    \n",
    "    df['game_date'] = pd.to_datetime(df['game_date'], format='%Y-%m-%d')\n",
    "    print('datetime done')\n",
    "    df['events'] = df['events'].apply(lambda x: 1 if x in hit_events else (0 if x in pa_ending_events else 0)).astype(np.int8)\n",
    "    print('events done')\n",
    "    df['batter_righty'] = df['stand'].apply(lambda x: 1 if x == 'R' else 0).astype(np.int8)\n",
    "    print('stand done')\n",
    "    df['pitcher_righty'] = df['p_throws'].apply(lambda x: 1 if x == 'R' else 0).astype(np.int8)\n",
    "    print('pitcher done')\n",
    "    df['bottom'] = df['inning_topbot'].apply(lambda x: 1 if x == 'Bot' else 0).astype(np.int8)\n",
    "    print('inning done')\n",
    "    df['release_speed'] = df['release_speed'].astype(np.int8)\n",
    "    df['batter'] = df['batter'].astype(np.int32)\n",
    "    df['pitcher'] = df['pitcher'].astype(np.int32)\n",
    "    df['zone'] = df['zone'].astype(np.int8)\n",
    "    df['hit_location'] = df['hit_location'].astype(np.int8)\n",
    "    df['balls'] = df['balls'].astype(np.uint8)\n",
    "    df['strikes'] = df['strikes'].astype(np.uint8)\n",
    "    df['game_year'] = df['game_year'].astype(np.int16)\n",
    "    df['on_3b'] = df['on_3b'].astype(np.int32)\n",
    "    df['on_2b'] = df['on_2b'].astype(np.int32)\n",
    "    df['on_1b'] = df['on_1b'].astype(np.int32)\n",
    "    df['outs_when_up'] = df['outs_when_up'].astype(np.uint8)\n",
    "    df['inning'] = df['inning'].astype(np.uint8)\n",
    "    df['fielder_2'] = df['fielder_2'].astype(np.int32)\n",
    "    df['hit_distance_sc'] = df['hit_distance_sc'].astype(np.int16)\n",
    "    df['release_spin_rate'] = df['release_spin_rate'].astype(np.int16)\n",
    "    df['game_pk'] = df['game_pk'].astype(np.int32)\n",
    "    df['pitcher.1'] = df['pitcher.1'].astype(np.int32)\n",
    "    df['fielder_2.1'] = df['fielder_2.1'].astype(np.int32)\n",
    "    df['fielder_3'] = df['fielder_3'].astype(np.int32)\n",
    "    df['fielder_4'] = df['fielder_4'].astype(np.int32)\n",
    "    df['fielder_5'] = df['fielder_5'].astype(np.int32)\n",
    "    df['fielder_6'] = df['fielder_6'].astype(np.int32)\n",
    "    df['fielder_7'] = df['fielder_7'].astype(np.int32)\n",
    "    df['fielder_8'] = df['fielder_8'].astype(np.int32)\n",
    "    df['fielder_9'] = df['fielder_9'].astype(np.int32)\n",
    "    df['woba_value'] = df['woba_value'].astype(np.int8)\n",
    "    df['woba_denom'] = df['woba_denom'].astype(np.int8)\n",
    "    df['babip_value'] = df['babip_value'].astype(np.int8)\n",
    "    df['at_bat_number'] = df['at_bat_number'].astype(np.uint8)\n",
    "    df['pitch_number'] = df['pitch_number'].astype(np.uint8)\n",
    "    df['home_score'] = df['home_score'].astype(np.uint8)\n",
    "    df['away_score'] = df['away_score'].astype(np.uint8)\n",
    "    df['bat_score'] = df['bat_score'].astype(np.uint8)\n",
    "    df['fld_score'] = df['fld_score'].astype(np.uint8)\n",
    "    df['post_away_score'] = df['post_away_score'].astype(np.uint8)\n",
    "    df['post_home_score'] = df['post_home_score'].astype(np.uint8)\n",
    "    df['post_bat_score'] = df['post_bat_score'].astype(np.uint8)\n",
    "    df['post_fld_score'] = df['post_fld_score'].astype(np.uint8)\n",
    "    df = df.select_dtypes(exclude=['object'])\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = set_types(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0cee85",
   "metadata": {},
   "source": [
    "Now to drop all rows that don't include a plate appearance ending play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83763bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['events'].isin([1,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['game_pk'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee4c67",
   "metadata": {},
   "source": [
    "At this point, purely for presentational purposes, I'm going to trim the dataframe down to the first 5000 rows. This will allow for the following code to run in a timely manner. The fact of the matter is with a project like this, there's a huge amount of data that could be included, making parts of what's to come run for literal days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67266d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795756c",
   "metadata": {},
   "source": [
    "Another factor I'd like to account for is both physical location (whether latitude, longitude, and altitude play a role) as well as weather. To add this in I'll need to map the games to the venue in statsapi.schedule() and then to coordinate data to run through an api called Visual Crossing for weather information. I'll also need to convert the times to local for Visual Crossing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94261e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_UTC_to_local(row):\n",
    "    venue_coords = pd.read_csv('Parks.csv')\n",
    "    \n",
    "    venue_name = row['venue_name']\n",
    "    game_datetime = row['game_datetime']\n",
    "    from_zone = tz.gettz('UTC')\n",
    "\n",
    "    locator = Nominatim(user_agent='myGeocoder')\n",
    "    try:\n",
    "        city = venue_coords[venue_coords['NAME'] == row['venue_name']]['CITY'].iloc[0]\n",
    "        lat = venue_coords[venue_coords['NAME'] == row['venue_name']]['Latitude'].iloc[0]\n",
    "        lon = venue_coords[venue_coords['NAME'] == row['venue_name']]['Longitude'].iloc[0]\n",
    "        alt = venue_coords[venue_coords['NAME'] == row['venue_name']]['Altitude'].iloc[0]\n",
    "    except NameError:\n",
    "        raise IndexError(venue, index)\n",
    "    if city == 'Tokyo':\n",
    "        to_zone = tz.gettz('Asia/Tokyo')\n",
    "    elif city == 'London':\n",
    "        to_zone = tz.gettz('Europe/London')\n",
    "    elif city in ['San Francisco',\n",
    "                        'Oakland',\n",
    "                        'Phoenix',\n",
    "                        'Seattle',\n",
    "                        'Los Angeles',\n",
    "                        'San Diego',\n",
    "                        'Anaheim']:\n",
    "        to_zone = tz.gettz('America/Los_Angeles')\n",
    "    elif city == 'Denver':\n",
    "        to_zone = tz.gettz('America/Denver')\n",
    "    elif city in ['Minneapolis',\n",
    "                        'Milwaukee',\n",
    "                        'Chicago',\n",
    "                        'St. Louis',\n",
    "                        'Arlington',\n",
    "                        'Kansas City',\n",
    "                        'Houston',\n",
    "                        'Monterrey',\n",
    "                        'Omaha',\n",
    "                        'Dyersville']:\n",
    "        to_zone = tz.gettz('America/Chicago')\n",
    "    elif city in ['Buffalo',\n",
    "                        'Detroit',\n",
    "                        'Cincinnati',\n",
    "                        'Pittsburgh',\n",
    "                        'Tampa Bay',\n",
    "                        'Philadelphia',\n",
    "                        'Atlanta',\n",
    "                        'New York',\n",
    "                        'Washington',\n",
    "                        'Cleveland',\n",
    "                        'Miami',\n",
    "                        'Boston',\n",
    "                        'Baltimore',\n",
    "                        'Toronto',\n",
    "                        'Williamsport',\n",
    "                        'Dunedin',\n",
    "                        'St. Petersburg']:\n",
    "        to_zone = tz.gettz('America/New_York')\n",
    "    else:\n",
    "        raise NameError(venue_name, city)\n",
    "    utc = datetime.strptime(game_datetime, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "    local = utc.astimezone(to_zone)\n",
    "\n",
    "    return(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_coords = pd.read_csv('Parks.csv')\n",
    "\n",
    "game_pk_df = pd.DataFrame(columns = statsapi.schedule(game_id=566083)[0].keys())\n",
    "\n",
    "venue_dict = {}\n",
    "\n",
    "# for each game, get the available data including probable pitchers\n",
    "for game in df_filtered['game_pk'].unique():\n",
    "    game_data = statsapi.schedule(game_id = int(game))[-1]\n",
    "    game_pk_df = pd.concat([game_pk_df, game_data], ignore_index=True)\n",
    "    print(game)\n",
    "\n",
    "try:\n",
    "    for venue in game_pk_df['venue_name'].unique():\n",
    "        print(venue)\n",
    "        city = venue_coords[venue_coords['NAME'] == venue]['CITY'].iloc[0]\n",
    "        lat = venue_coords[venue_coords['NAME'] == venue]['Latitude'].iloc[0]\n",
    "        lon = venue_coords[venue_coords['NAME'] == venue]['Longitude'].iloc[0]\n",
    "        alt = venue_coords[venue_coords['NAME'] == venue]['Altitude'].iloc[0]\n",
    "        venue_dict[venue] = np.array((city, lat, lon, alt))\n",
    "except IndexError:\n",
    "#     In case there's a new venue name and Parks.csv needs to be updated\n",
    "    raise NameError(venue, index)\n",
    "\n",
    "game_pk_df = game_pk_df.sort_values(['game_datetime'])\n",
    "\n",
    "\n",
    "# convert UTC time to local time\n",
    "game_pk_df['local_datetime'] = game_pk_df.apply(lambda row: convert_UTC_to_local(row), axis=1)\n",
    "game_pk_df['local_datetime'] = game_pk_df['local_datetime'].apply(lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "\n",
    "# create a dictionary to map each start time with the associated venues to cut down on api calls\n",
    "datetime_coordinate_matching = {}\n",
    "for index, row in game_pk_df.iterrows():\n",
    "    city, lat, lon, alt = venue_dict[row['venue_name']]\n",
    "    dc_datetime = row['local_datetime']\n",
    "    if dc_datetime in datetime_coordinate_matching.keys():\n",
    "        datetime_coordinate_matching[dc_datetime].append(','.join([str(lat), str(lon)]))\n",
    "    else:\n",
    "        datetime_coordinate_matching[dc_datetime] = [','.join([str(lat), str(lon)])]\n",
    "\n",
    "game_pk_df['coordinates'] = game_pk_df['venue_name'].apply(lambda x: ','.join(venue_dict[x][1:3]))\n",
    "game_pk_df['alt'] = game_pk_df['venue_name'].apply(lambda x: venue_dict[x][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_pk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "visual_crossing = os.environ.get('VISUAL_CROSSING_API_KEY', '')\n",
    "if not visual_crossing:\n",
    "    print(\"Warning: VISUAL_CROSSING_API_KEY not set. Weather data fetching will fail.\")\n",
    "import requests\n",
    "\n",
    "api_counter = 0\n",
    "\n",
    "weather_df = pd.DataFrame()\n",
    "for key, value in datetime_coordinate_matching.items():\n",
    "    print(key)\n",
    "    url_locations = '|'.join(value)\n",
    "    URL = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?&aggregateHours=1&startDateTime={key}&endDateTime={key}&unitGroup=us&contentType=json&location={url_locations}&key={visual_crossing}'\n",
    "    global api_counter\n",
    "#     api is limited to 1000 free calls per day, after which it's $0.0001 per result\n",
    "    if api_counter == 1000:\n",
    "        print('Prepreprocessing will continue at ' + (datetime.now()+timedelta(seconds = 86400)).strftime('%H:%M:%S'))\n",
    "        sleep(86400)\n",
    "        api_counter = 0\n",
    "    try:\n",
    "        api_counter += 1\n",
    "        response = requests.get(URL)\n",
    "        data = response.json()\n",
    "        locations = list(data['locations'].keys())\n",
    "        for each in locations:\n",
    "            values = data['locations'][each]['values'][0]\n",
    "            weather_df = pd.concat([weather_df, pd.DataFrame([{**{'coordinates': each}, **values}])], ignore_index=True)\n",
    "    except:\n",
    "        raise IndexError(key, value, response)\n",
    "\n",
    "weather_df['datetimeStr'] = weather_df['datetimeStr'].apply(lambda x: x[:-6])\n",
    "weather_df = weather_df.rename(columns={'datetimeStr': 'local_datetime'})\n",
    "\n",
    "# merge all this new data together\n",
    "\n",
    "games_and_weather = pd.merge(\n",
    "    game_pk_df,\n",
    "    weather_df,\n",
    "    how=\"left\",\n",
    "    on=None,\n",
    "    left_on=['local_datetime', 'coordinates'],\n",
    "    right_on=['local_datetime', 'coordinates'],\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_gpk\", \"_acw\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")\n",
    "\n",
    "df_detailed = pd.merge(\n",
    "    games_and_weather,\n",
    "    df_filtered,\n",
    "    how=\"right\",\n",
    "    on=None,\n",
    "    left_on='game_id',\n",
    "    right_on='game_pk',\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_gaw\", \"_dff\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23414231",
   "metadata": {},
   "source": [
    "There's a couple more pieces of information I'd like to add in, namely whether a game was planned to be only 7 innings long, as is true in doubleheaders in 2020 and 2021 (due to COVID), and whether a designated hitter is used, which could indicate longer outings for the starting pitcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbcd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "american_league_teams = np.array(['Boston Red Sox', 'Houston Astros', 'Chicago White Sox', 'Tampa Bay Rays', 'Oakland Athletics', 'Seattle Mariners', 'New York Yankees', 'Toronto Blue Jays', 'Los Angeles Angels', 'Cleveland Indians', 'Detroit Tigers', 'Kansas City Royals', 'Minnesota Twins', 'Texas Rangers', 'Baltimore Orioles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f810f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed['covid_doubleheader'] = df_detailed.apply(lambda row: 1 if row['game_year'] in [2020, 2021] and row['doubleheader'] =='Y' else 0, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_detailed['designated_hitter'] = df_detailed.apply(lambda row: 1 if row['home_name'] in american_league_teams or row['game_year'] == 2020 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c7f7c",
   "metadata": {},
   "source": [
    "And we'll save our dataframe here in case something were to go wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a810d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed.to_csv('df_detailed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cf233",
   "metadata": {},
   "source": [
    "At this point I have a good amount of data but I'm skeptical any model I use would be able to understand who is a good hitter or pitcher, two factors that would likely be extremely relevant. To make up for this I'm going to assemble a handful of statistics for each player based on their performance in recent history - the previous 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = statcast('2017-01-01', '2017-12-31')\n",
    "\n",
    "# data.to_csv('untouched_2017_statcast_pbp.csv')\n",
    "\n",
    "\n",
    "\n",
    "# data = statcast('2018-01-01', '2018-12-31')\n",
    "\n",
    "# data.to_csv('untouched_2018_statcast_pbp.csv')\n",
    "\n",
    "history = pd.concat([\n",
    "                    set_types(pd.read_csv('untouched_2017_statcast_pbp.csv')),\n",
    "                   set_types(pd.read_csv('untouched_2018_statcast_pbp.csv')),\n",
    "                   set_types(pd.read_csv('untouched_2019_statcast_pbp.csv')),\n",
    "                   set_types(pd.read_csv('untouched_2020_statcast_pbp.csv')),\n",
    "                   set_types(pd.read_csv('untouched_2021_statcast_pbp.csv'))\n",
    "                    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_cumulative_stats(player, stats_type, history, start_date=None, end_date=datetime.strptime('2021-06-30', '%Y-%m-%d')):\n",
    "    print(end_date)\n",
    "    print(player)\n",
    "\n",
    "#     in order to speed this up and cut down on api calls, I'll simply be slicing a collection of all statcast data since 2017 (2 years prior to the earliest data I'm modeling)\n",
    "\n",
    "    if start_date == None:\n",
    "        start_date = end_date-timedelta(weeks=104)\n",
    "    if type(player) == str:\n",
    "#         playerid_lookup has trouble accounting for 3 names\n",
    "        if player[-3:] == 'Jr.':\n",
    "            player = player[:-4]\n",
    "        player = player.split(' ')\n",
    "        player.reverse()\n",
    "        try:\n",
    "            player_info = pybaseball.playerid_lookup(player[0], player[-1])\n",
    "            if len(player_info) >1:\n",
    "                player_info = player_info.iloc[0]\n",
    "                player = player_info['key_mlbam']\n",
    "            elif player_info.empty:\n",
    "                player_info = pybaseball.playerid_lookup(player[0], player[-1], fuzzy=True).iloc[0]\n",
    "                player = player_info['key_mlbam']\n",
    "            else:\n",
    "                try:\n",
    "                    player = player_info.iloc[0]['key_mlbam']\n",
    "                except:\n",
    "                    raise NameError(player_info, player)\n",
    "        except:\n",
    "            raise NameError(player)\n",
    "            \n",
    "    history = history[(history['game_date'] >= start_date) & (history['game_date'] <= end_date)]\n",
    "    print('game_date filtered')\n",
    "    if stats_type == 'pitcher':\n",
    "        try:\n",
    "            history = history[history['pitcher'] == player]\n",
    "        except:\n",
    "            raise NameError(player)\n",
    "        print('player filtered')\n",
    "    #   PITCH METRICS\n",
    "        pitches = {}\n",
    "        while True:\n",
    "            try:\n",
    "                pitches['pitch_hand'] = statsapi.player_stat_data(player)['pitch_hand']\n",
    "                break\n",
    "            except:\n",
    "                print('pitch_hand_error')\n",
    "                sleep(10)\n",
    "        print('pitch_hand')\n",
    "    #     filter history down to PA-enders\n",
    "        history = history[history['events'].isin([1, 0])]\n",
    "        print('pa ending events')\n",
    "    #    PLAYER METRICS\n",
    "        games = history['game_pk'].unique()\n",
    "        at_bat_list = []\n",
    "        num_hits_list = []\n",
    "        for game in games:\n",
    "            game = history[history['game_pk'] == game]\n",
    "            inning = game['inning'].max()\n",
    "            at_bats = len(game)\n",
    "            at_bat_list.append(at_bats)\n",
    "            for i in range(1, inning+1):\n",
    "                num_hits = len(game[(game['inning'] == i) & (game['events'] == 1)])\n",
    "                num_hits_list.append(num_hits)\n",
    "        pitches['games_played_last_2_years_pitcher'] = len(games)\n",
    "        pitches['avg_PAs_per_apparence_pitcher'] = np.array(at_bat_list).mean()\n",
    "        pitches['avg_hits_per_inning'] = np.array(num_hits_list).mean()\n",
    "\n",
    "#         L/R splits\n",
    "        left_pitcher = history[history['batter_righty'] == 0]\n",
    "        right_pitcher = history[history['batter_righty'] == 1]\n",
    "        l_pas = len(left_pitcher)\n",
    "        r_pas = len(right_pitcher)\n",
    "        l_hits = len(left_pitcher[(history['events'] == 1)])\n",
    "        r_hits = len(right_pitcher[(history['events'] == 1)])\n",
    "\n",
    "        if l_pas > 0 and r_pas > 0:\n",
    "            pitches['H/PA_pitcher'] = (l_hits + r_hits)/(l_pas + r_pas)\n",
    "            pitches['against_lefties_H/PA'] = (l_hits)/(l_pas)\n",
    "            pitches['against_righties_H/PA'] = (r_hits)/(r_pas)\n",
    "        elif l_pas > 0:\n",
    "            pitches['H/PA_pitcher'] = (l_hits + r_hits)/(l_pas + r_pas)\n",
    "            pitches['against_lefties_H/PA'] = (l_hits)/(l_pas)\n",
    "            pitches['against_righties_H/PA'] = 0\n",
    "        elif r_pas > 0:\n",
    "            pitches['H/PA_pitcher'] = (l_hits + r_hits)/(l_pas + r_pas)\n",
    "            pitches['against_lefties_H/PA'] = 0\n",
    "            pitches['against_righties_H/PA'] = (r_hits)/(r_pas)\n",
    "        else:\n",
    "            pitches['H/PA_pitcher'] = 0\n",
    "            pitches['against_lefties_H/PA'] = 0\n",
    "            pitches['against_righties_H/PA'] = 0\n",
    "        return(pitches)\n",
    "\n",
    "    \n",
    "    elif stats_type == 'batter':\n",
    "        history = history[history['batter'] == player]\n",
    "\n",
    "\n",
    "#     filter history down to PA-enders\n",
    "        history = history[history['events'].isin([1, 0])]\n",
    "\n",
    "#     H/PA per pitch type\n",
    "#     K/PA per pitch type\n",
    "#     H/PA vs righties, lefties\n",
    "#     PA/G\n",
    "#     avg_launch_angle\n",
    "#     avg_launch_speed\n",
    "#     xBA based on estimated_ba_using_speedangle\n",
    "\n",
    "\n",
    "\n",
    "        # pitches = history['pitch_type'].unique()\n",
    "        games = history['game_pk'].unique()\n",
    "        pas_list = []\n",
    "\n",
    "        batter = {}\n",
    "\n",
    "        for game in games:\n",
    "            pas = len(history[history['game_pk'] == game])\n",
    "            # print(pas)\n",
    "            pas_list.append(pas)\n",
    "        batter['games_played_last_2_years_batter'] = len(games)\n",
    "        batter['PA/G_batter'] = np.array(pas_list).mean()\n",
    "\n",
    "#         righty/lefty split\n",
    "        pitcher_right = history[history['pitcher_righty'] == 1]\n",
    "        pas_r = len(pitcher_right)\n",
    "        hits_r = len(pitcher_right[pitcher_right['events'] == 1])\n",
    "\n",
    "        if pas_r > 0:\n",
    "            batter['H/PA_against_R'] = hits_r/pas_r\n",
    "        else:\n",
    "            batter['H/PA_against_R'] = 0\n",
    "\n",
    "        pitcher_left = history[history['pitcher_righty'] == 0]\n",
    "        pas_l = len(pitcher_left)\n",
    "        hits_l = len(pitcher_left[pitcher_left['events'] == 1])\n",
    "\n",
    "        if pas_l > 0:\n",
    "            batter['H/PA_against_L'] = hits_l/pas_l\n",
    "        else:\n",
    "            batter['H/PA_against_L'] = 0\n",
    "\n",
    "        if pas_r > 0 or pas_l > 0:\n",
    "            batter['H/PA_batter'] = (hits_r+hits_l)/(pas_r+pas_l)\n",
    "        else:\n",
    "            batter['H/PA_batter'] = 0\n",
    "        batter['avg_launch_angle'] = history['launch_angle'].mean()\n",
    "        batter['avg_launch_speed'] = history['launch_speed'].mean()\n",
    "        batter['xBA'] = history['estimated_ba_using_speedangle'].mean()\n",
    "\n",
    "        return(batter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf7679",
   "metadata": {},
   "source": [
    "We'll run each projected starting pitcher through the function, derived_cumulative_stats, to assemble their time adjusted statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitcher_df = pd.concat([df_detailed[['game_id', 'game_date_gaw', 'away_probable_pitcher']].rename(columns={'away_probable_pitcher': 'probable_pitcher'}), df_detailed[['game_id', 'game_date_gaw', 'home_probable_pitcher']].rename(columns={'home_probable_pitcher': 'probable_pitcher'})])\n",
    "pitcher_df = pitcher_df.drop_duplicates()\n",
    "pitcher_df['probable_pitcher'] = pitcher_df['probable_pitcher'].replace('', np.nan)\n",
    "pitcher_df = pitcher_df.dropna(subset=['probable_pitcher'])\n",
    "pitcher_df = pitcher_df.reset_index(drop=True)\n",
    "\n",
    "pitcher_stats = pitcher_df.apply(lambda row: pd.Series(derived_cumulative_stats(row['probable_pitcher'], 'pitcher', history, end_date=datetime.strptime(row['game_date_gaw'], '%Y-%m-%d')-timedelta(days=1))), axis = 1)\n",
    "\n",
    "pitcher_df = pd.merge(\n",
    "    pitcher_stats,\n",
    "    pitcher_df,\n",
    "    how=\"left\",\n",
    "    on=None,\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    sort=False,\n",
    "    suffixes=(\"_s\", \"_df\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa05fee",
   "metadata": {},
   "source": [
    "In order to properly pair batters with their pitcher we'll need to split home and away batters and re-merge those dataframes by matching them with their proper pitcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_batters = df_detailed[df_detailed['bottom'] == 1]\n",
    "away_batters = df_detailed[df_detailed['bottom'] == 0]\n",
    "\n",
    "df_wpitching_p1 = pd.merge(\n",
    "    pitcher_df,\n",
    "    home_batters,\n",
    "    how=\"right\",\n",
    "    on=None,\n",
    "    left_on=['game_id', 'probable_pitcher'],\n",
    "    right_on=['game_id', 'away_probable_pitcher'],\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_p\", \"_df\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")\n",
    "\n",
    "df_wpitching_p2 = pd.merge(\n",
    "    pitcher_df,\n",
    "    away_batters,\n",
    "    how=\"right\",\n",
    "    on=None,\n",
    "    left_on=['game_id', 'probable_pitcher'],\n",
    "    right_on=['game_id', 'home_probable_pitcher'],\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_p\", \"_df\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")\n",
    "df_wpitching = pd.concat([df_wpitching_p1, df_wpitching_p2], ignore_index=True)\n",
    "df_wpitching = df_wpitching.sort_values(['game_date_gaw_df', 'game_pk','home_name', 'inning', 'bottom', 'outs_when_up', 'at_bat_number', 'pitch_number']).reset_index(drop=True)\n",
    "\n",
    "#         Makes explicit the target value and drops repeat occurences of the target value in each game played\n",
    "\n",
    "df_wpitching['got_a_hit'] = df_wpitching.apply(lambda row: 1 if row['events'] == 1 else 0, axis=1)\n",
    "df_wpitching = df_wpitching.sort_values('got_a_hit').drop_duplicates(subset=['game_id', 'batter'], keep='last').sort_values(['game_date_gaw_df', 'game_pk','home_name', 'inning', 'bottom', 'outs_when_up', 'at_bat_number', 'pitch_number']).reset_index(drop=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a3ef6",
   "metadata": {},
   "source": [
    "At this point it's safe to run our batters through the batter half of the derived_cumulative_stats function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpitching.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_stats = df_wpitching.apply(lambda row: pd.Series(derived_cumulative_stats(row['batter'], 'batter', history, end_date=datetime.strptime(row['game_date_gaw_df'], '%Y-%m-%d')-timedelta(days=1))), axis = 1)\n",
    "\n",
    "\n",
    "df_final = pd.merge(\n",
    "    batting_stats,\n",
    "    df_wpitching,\n",
    "    how=\"right\",\n",
    "    on=None,\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    sort=False,\n",
    "    suffixes=(\"_bs\", \"_df\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a379d2",
   "metadata": {},
   "source": [
    "Looks like we have just a few things to clean up as well as a number of columns that we wouldn't know ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fded192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.rename(columns={'games_played_last_2_years_df': 'games_played_last_2_years_pitcher', 'games_played_last_2_years_bs': 'games_played_last_2_years_batter', 'bottom': 'home'})\n",
    "\n",
    "df_final['local_datetime'] = pd.to_datetime(df_final['local_datetime'])\n",
    "df_final['local_date'] = df_final['local_datetime'].apply(lambda x: x.toordinal())\n",
    "df_final['local_datetime'] = df_final['local_datetime'].apply(lambda x: x.timestamp())\n",
    "\n",
    "columns_to_drop = ['player_name',\n",
    " 'game_type_dff', 'pitch_type',\n",
    " 'release_speed',\n",
    " 'release_pos_x',\n",
    " 'release_pos_z',\n",
    " 'pitcher',\n",
    " 'events',\n",
    " 'description',\n",
    " 'spin_dir',\n",
    "'spin_rate_deprecated',\n",
    " 'break_angle_deprecated',\n",
    " 'break_length_deprecated',\n",
    " 'zone',\n",
    " 'des',\n",
    "  'p_throws',\n",
    "'type',\n",
    " 'hit_location',\n",
    " 'bb_type',\n",
    " 'balls',\n",
    " 'strikes',\n",
    " 'pfx_x',\n",
    " 'pfx_z',\n",
    " 'plate_x',\n",
    " 'plate_z',\n",
    " 'on_3b',\n",
    " 'on_2b',\n",
    " 'on_1b',\n",
    " 'outs_when_up',\n",
    " 'inning',\n",
    " 'hc_x',\n",
    " 'hc_y',\n",
    " 'tfs_deprecated',\n",
    " 'tfs_zulu_deprecated',\n",
    "  'sv_id',\n",
    " 'vx0',\n",
    " 'vy0',\n",
    " 'vz0',\n",
    " 'ax',\n",
    " 'ay',\n",
    " 'az',\n",
    " 'sz_top',\n",
    " 'sz_bot',\n",
    " 'hit_distance_sc',\n",
    " 'launch_speed',\n",
    " 'launch_angle',\n",
    " 'effective_speed',\n",
    " 'release_spin_rate',\n",
    " 'release_extension',\n",
    " 'pitcher.1',\n",
    " 'fielder_2.1',\n",
    " 'fielder_3',\n",
    " 'fielder_4',\n",
    " 'fielder_5',\n",
    " 'fielder_6',\n",
    " 'fielder_7',\n",
    " 'fielder_8',\n",
    " 'fielder_9',\n",
    " 'release_pos_y',\n",
    " 'estimated_ba_using_speedangle',\n",
    " 'estimated_woba_using_speedangle',\n",
    " 'woba_value',\n",
    " 'woba_denom',\n",
    " 'babip_value',\n",
    " 'iso_value',\n",
    " 'launch_speed_angle',\n",
    " 'at_bat_number',\n",
    " 'pitch_number',\n",
    " 'pitch_name',\n",
    " 'home_score_dff',\n",
    " 'away_score_dff',\n",
    " 'bat_score',\n",
    " 'fld_score',\n",
    " 'post_away_score',\n",
    " 'post_home_score',\n",
    " 'post_bat_score',\n",
    " 'post_fld_score',\n",
    " 'if_fielding_alignment',\n",
    " 'of_fielding_alignment',\n",
    " 'spin_axis',\n",
    " 'delta_home_win_exp',\n",
    " 'delta_run_exp',\n",
    " 'game_pk',\n",
    " 'game_id','game_date_gaw_p',\n",
    " 'game_datetime',\n",
    " 'game_date_gaw_df',\n",
    " 'game_type_gaw',\n",
    " 'datetime',\n",
    "#  'datetimeStr',\n",
    " 'game_date_dff','winning_team',\n",
    " 'losing_team',\n",
    " 'winning_pitcher',\n",
    " 'losing_pitcher',\n",
    " 'save_pitcher',\n",
    " 'summary', 'home_probable_pitcher',\n",
    " 'away_probable_pitcher',\n",
    " 'home_pitcher_note',\n",
    " 'away_pitcher_note',\n",
    " 'away_score_gaw',\n",
    " 'home_score_gaw',\n",
    " 'current_inning',\n",
    " 'inning_state',\n",
    " 'venue_id',\n",
    " 'status',\n",
    " 'home_team', 'away_team', 'home_id', 'away_id']\n",
    "\n",
    "for col in df_final.columns:\n",
    "    if col in columns_to_drop:\n",
    "        df_final= df_final.drop([col], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d4877",
   "metadata": {},
   "source": [
    "# MODEL WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a929c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, plot_confusion_matrix, precision_score, plot_roc_curve, make_scorer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392c772",
   "metadata": {},
   "source": [
    "First I wrote a function to return certain traditional statistical scores for my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b425e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(estimator, X_train, X_val, y_train, y_val, holdout, roc_auc='proba', output = False):\n",
    "\n",
    "    X_holdout = holdout.drop(['got_a_hit'], axis=1)\n",
    "    y_holdout = holdout['got_a_hit']\n",
    "    \n",
    "    #     grab predictions\n",
    "    train_preds = estimator.predict(X_train)\n",
    "    val_preds = estimator.predict(X_val)\n",
    "    holdout_preds = estimator.predict(X_holdout)\n",
    "    \n",
    "#     output needed for roc_auc score\n",
    "    if roc_auc == 'skip':\n",
    "        train_out = False\n",
    "        val_out = False\n",
    "        holdout_out = False\n",
    "    elif roc_auc == 'dec': # not all classifiers have a decision function\n",
    "        train_out = estimator.decision_function(X_train)\n",
    "        val_out = estimator.decision_function(X_val)\n",
    "        holdout_out = estimator.decision_function(X_holdout)\n",
    "    elif roc_auc == 'proba':\n",
    "        try:\n",
    "            train_out = estimator.predict_proba(X_train)[:, 1]\n",
    "            val_out = estimator.predict_proba(X_val)[:, 1]\n",
    "            holdout_out = estimator.predict_proba(X_holdout)[:, 1]\n",
    "        except AttributeError:\n",
    "            train_out = estimator.predict(X_train)\n",
    "            val_out = estimator.predict(X_val)\n",
    "            holdout_out = estimator.predict(X_holdout)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"The value for roc_auc should be 'skip', 'dec', or 'proba'.\")\n",
    "    \n",
    "    ac = accuracy_score(y_train, train_preds)\n",
    "    f1 = f1_score(y_train, train_preds)\n",
    "    ras = roc_auc_score(y_train, train_out)\n",
    "    pr = precision_score(y_train, train_preds)\n",
    "    \n",
    "    if output == True:\n",
    "        print('Train Scores')\n",
    "        print('------------')\n",
    "        print(f'Accuracy: {ac}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        if type(train_out) == np.ndarray:\n",
    "            print(f'ROC-AUC: {ras}')\n",
    "        print(f'Precision: {pr}')\n",
    "\n",
    "    ac = accuracy_score(y_val, val_preds)\n",
    "    f1 = f1_score(y_val, val_preds)\n",
    "#     print(type(y_val))\n",
    "    ras = roc_auc_score(y_val, val_out)\n",
    "    pr = precision_score(y_val, val_preds)\n",
    "    \n",
    "    if output == True:\n",
    "        print('-----------------------------------')\n",
    "        print('Val Scores')\n",
    "        print('-----------')\n",
    "        print(f'Accuracy: {ac}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        if type(val_out) == np.ndarray:\n",
    "            print(f'ROC-AUC: {ras}')\n",
    "        print(f'Precision: {pr}')\n",
    "    \n",
    "    ac = accuracy_score(y_holdout, holdout_preds)\n",
    "    f1 = f1_score(y_holdout, holdout_preds)\n",
    "#     print(type(holdout['got_a_hit']))\n",
    "    ras = roc_auc_score(y_holdout, holdout_out)\n",
    "    pr = precision_score(y_holdout, holdout_preds)\n",
    "    \n",
    "    if output == True:\n",
    "        print('-----------------------------------')\n",
    "        print('Holdout Scores')\n",
    "        print('-----------')\n",
    "        print(f'Accuracy: {ac}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "        if type(holdout_out) == np.ndarray:\n",
    "            print(f'ROC-AUC: {ras}')\n",
    "        print(f'Precision: {pr}')\n",
    "\n",
    "        print('\\nVal Data')\n",
    "        print('-----------')\n",
    "\n",
    "        plot_confusion_matrix(estimator, X_val, y_val, values_format=',.5g')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Holdout Data')        \n",
    "        print('-----------')\n",
    "\n",
    "        plot_confusion_matrix(estimator, X_holdout, y_holdout, values_format=',.5g')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996891",
   "metadata": {},
   "source": [
    "Then I wrote one to measure hitting streaks which in this particular case is the most relevant metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad00f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streak_checker(dataframe, output = False, metric = 'odds', ascending=False, combiner=None, timed=False, names=False):\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        if 'L/R_split' in metric and combiner == 'add':\n",
    "            if timed == True:\n",
    "                start = datetime.now()\n",
    "\n",
    "                print(combiner, start)\n",
    "            dataframe_copy = dataframe.copy()\n",
    "            dataframe_copy['temp'] = dataframe.apply(lambda row: row['H/PA_against_R'] + row['against_lefties_H/PA'] if (row['pitcher_righty'] == 1 and row['batter_righty'] == 0) else (row['H/PA_against_L'] + row['against_righties_H/PA'] if (row['pitcher_righty'] == 0 and row['batter_righty'] == 1) else 0), axis=1)\n",
    "            mms = MinMaxScaler()\n",
    "            dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "            \n",
    "            if timed == True:\n",
    "                print(datetime.now() - start)\n",
    "            dataframe = dataframe_copy\n",
    "            metric='temp'\n",
    "        elif 'L/R_split' in metric and combiner == 'multiply':\n",
    "            if timed == True:\n",
    "                start = datetime.now()\n",
    "\n",
    "                print(combiner, start)\n",
    "            dataframe_copy = dataframe.copy()\n",
    "            dataframe_copy['temp'] = dataframe.apply(lambda row: row['H/PA_against_R'] * row['against_lefties_H/PA'] if (row['pitcher_righty'] == 1 and row['batter_righty'] == 0) else (row['H/PA_against_L'] * row['against_righties_H/PA'] if (row['pitcher_righty'] == 0 and row['batter_righty'] == 1) else 0), axis=1)\n",
    "            mms = MinMaxScaler()\n",
    "            dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "            \n",
    "            if timed == True:\n",
    "                print(datetime.now() - start)\n",
    "            dataframe = dataframe_copy\n",
    "            metric='temp'\n",
    "        if type(metric) == list:\n",
    "            dataframe_copy = dataframe.copy()\n",
    "            if combiner == 'add':\n",
    "                if timed == True:\n",
    "                    start = datetime.now()\n",
    "\n",
    "                    print(combiner, start)\n",
    "                dataframe_copy['temp'] = dataframe.apply(lambda row: row[metric[0]] + row[metric[1]], axis=1)\n",
    "                mms = MinMaxScaler()\n",
    "                dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "                \n",
    "                if timed == True:\n",
    "                    print(datetime.now() - start)\n",
    "            elif combiner == 'multiply':\n",
    "                if timed == True:\n",
    "                    start = datetime.now()\n",
    "\n",
    "                    print(combiner, start)\n",
    "                dataframe_copy['temp'] = dataframe.apply(lambda row: row[metric[0]] * row[metric[1]], axis=1)\n",
    "                mms = MinMaxScaler()\n",
    "                dataframe_copy['temp'] = pd.DataFrame(mms.fit_transform(dataframe_copy[['temp']]))\n",
    "                \n",
    "                if timed == True:\n",
    "                    print(datetime.now() - start)\n",
    "            \n",
    "            else:\n",
    "                raise KeyError(f'{combiner} combiners have not been implemented yet')\n",
    "            dataframe = dataframe_copy\n",
    "            metric = 'temp'\n",
    "        else:\n",
    "            pass\n",
    "    except KeyError:\n",
    "        raise KeyError(\"metric must be chosen from dataframe's columns\")\n",
    "    \n",
    "    date_list = np.sort(dataframe['local_date'].unique())\n",
    "    \n",
    "    for day in date_list:\n",
    "        best_bet = dataframe[dataframe['local_date'] == day].sort_values(metric, ascending=ascending).iloc[0]\n",
    "        \n",
    "        odds = best_bet[metric]\n",
    "        date = datetime.fromordinal(best_bet['local_date']).strftime(\"%b %d %Y\")\n",
    "        \n",
    "        \n",
    "        if best_bet['home'] == 0:\n",
    "            team = best_bet['away_name']\n",
    "        else:\n",
    "            team = best_bet['home_name']\n",
    "    #     team = statsapi.lookup_team(player['currentTeam']['id'])[0]['name']\n",
    "    #     player_name = player['fullName']\n",
    "        actual_result = best_bet['got_a_hit']\n",
    "        results.append(actual_result)\n",
    "        if output == True:\n",
    "            if day > date_list[-20] and names==True:\n",
    "#                 pass\n",
    "                player_info = pybaseball.playerid_reverse_lookup([best_bet['batter']])\n",
    "                if len(player_info) >1:\n",
    "                    player_info = player_info.iloc[0]\n",
    "                    player = player_info['key_mlbam']\n",
    "                elif player_info.empty:\n",
    "                    player_info = playerid_lookup(player[0], player[1], fuzzy=True).iloc[0]\n",
    "                    player = player_info['key_mlbam']\n",
    "                else:\n",
    "                    try:\n",
    "                        player = ' '.join(player_info.iloc[0][['name_first', 'name_last']])\n",
    "                    except:\n",
    "                        raise NameError(player_info, player)\n",
    "                print(date, player, round(odds, 2), actual_result)\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for num in results:\n",
    "        if num == 1:\n",
    "            current += 1\n",
    "            longest = max(longest, current)\n",
    "        else:\n",
    "            longest = max(longest, current)\n",
    "            current = 0\n",
    "    if output == True:\n",
    "        print(f'Longest streak in set ({len(results)} days): ', longest)\n",
    "        print(f'Total correct guesses in set ({len(results)} days): ', sum(results), sum(results)/len(results))\n",
    "#     return (dataframe, best_bet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87898028",
   "metadata": {},
   "source": [
    "And finally a function to streamline running the various different types of models, including a rough estimate of how long a model will take to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(clf, X_train, X_val, y_train, y_val, holdout):\n",
    "#     probability = True\n",
    "    \n",
    "#     clf.fit(X_train.head(100), y_train.head(100))\n",
    "\n",
    "#     evaluate(clf, X_train.head(100), X_val.head(100), y_train.head(100), y_val.head(100), holdout);\n",
    "\n",
    "#     runtime = datetime.now() - start\n",
    "\n",
    "#     if runtime > timedelta(seconds=5):\n",
    "#         print(f'Model would take {runtime.total_seconds()*200/60} minutes to run.')\n",
    "#     else:\n",
    "#         print('Model should take between {:.0f} and {:.0f} minutes to run and finish by {}'.format(runtime.total_seconds()*20/60, runtime.total_seconds()*200/60, (datetime.now()+(timedelta(seconds=(runtime.total_seconds()*200)))).strftime(\"%H:%M\")))\n",
    "    clf.fit(X_train, y_train)\n",
    "    evaluate(clf, X_train, X_val, y_train, y_val, holdout, output=True)\n",
    "    plot_roc_curve(clf, X_val, y_val)\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        val_df_odds = pd.Series(clf.predict_proba(X_val)[:, 1], name='odds')\n",
    "        holdout_df_odds = pd.Series(clf.predict_proba(holdout.drop(['got_a_hit'], axis=1))[:, 1], name='odds')\n",
    "    except AttributeError:\n",
    "        probability = False\n",
    "        val_df_odds = pd.Series(clf.predict(X_val), name= 'odds')\n",
    "        holdout_df_odds = pd.Series(clf.predict(holdout.drop(['got_a_hit'], axis=1)), name='odds')\n",
    "\n",
    "    val_df = X_val.assign(got_a_hit = y_val).reset_index(drop=True)\n",
    "    val_df = val_df.assign(odds=val_df_odds)\n",
    "    holdout = holdout.reset_index(drop=True)\n",
    "    holdout = holdout.assign(odds=holdout_df_odds)\n",
    "    if set(val_df_odds.value_counts().index) == set([1, 0]):\n",
    "        print(str(clf.get_params()['steps'][1][1]), \"doesn't return probabilities, so no streak results will be returned based on this alone.\")\n",
    "        pass\n",
    "    else:\n",
    "        print('-----------------------------------')\n",
    "\n",
    "        print('Val data')\n",
    "        print('-----------')\n",
    "\n",
    "        streak_checker(val_df, output=True)\n",
    "        print('Holdout data')\n",
    "        print('-----------')\n",
    "\n",
    "        streak_checker(holdout, output=True)\n",
    "\n",
    "    print('Odds multiplied by PA/G_batter')\n",
    "    print('-----------')\n",
    "\n",
    "    print('Val data')\n",
    "    print('-----------')\n",
    "    streak_checker(val_df, output=True, metric=['PA/G_batter', 'odds'], combiner='multiply')\n",
    "    print('Holdout data')\n",
    "    print('-----------')\n",
    "    streak_checker(holdout, output=True, metric=['PA/G_batter', 'odds'], combiner='multiply')\n",
    "\n",
    "    print('Odds added to PA/G_batter')\n",
    "    print('-----------')\n",
    "\n",
    "    print('Val data')\n",
    "    print('-----------')\n",
    "    streak_checker(val_df, output=True, metric=['L/R_split', 'odds'], combiner='add')\n",
    "    print('Holdout data')\n",
    "    print('-----------')\n",
    "    streak_checker(holdout, output=True, metric=['L/R_split', 'odds'], combiner='add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96b23d",
   "metadata": {},
   "source": [
    "Also, I made a custom train_test_split function. I realized a traditional one with a random split wouldn't be as effective for my particular type of data as one that made a split based on date. This is because the best candidate for a hit on any given day could be randomly put into one grouping or another, altering my results for no clear analytical gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510db092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=.25):\n",
    "    \n",
    "    '''My simplified train_test_split which just cuts the dataframe by date, in order to accurately evaluate streak results, to roughly the test size requested'''\n",
    "    \n",
    "    sample_size = len(df)\n",
    "    train_size = 1 - test_size\n",
    "    train_size = round(train_size * sample_size)\n",
    "    df = df.sort_values(['local_date']).reset_index(drop=True)\n",
    "    \n",
    "    date_cutoff = df.iloc[train_size]['local_date']\n",
    "    train = df[df['local_date'] <= date_cutoff]\n",
    "    val = df[df['local_date'] > date_cutoff]\n",
    "    \n",
    "    X_train = train.drop(['got_a_hit'], axis=1)\n",
    "    y_train = train['got_a_hit']\n",
    "    \n",
    "    X_val = val.drop(['got_a_hit'], axis=1)\n",
    "    y_val = val['got_a_hit']\n",
    "    \n",
    "    return(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0feebb",
   "metadata": {},
   "source": [
    "For this section, I'll be importing my previously organized sample and holdout sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample', index_col=1)\n",
    "holdout = pd.read_csv('holdout', index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fea5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107421e6",
   "metadata": {},
   "source": [
    "I decided to fill any missing numerical values with the feature's median, as this seemed the best way to minimize undue bias. Missing values used for my one hot encoder will be replaced with \"Unknown.\"  Finally for my categorical features, I decided on target encoding as it seemed most sensible to align the values with their relationship to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [] # all numeric columns\n",
    "cols_to_ohe = [] # doubleheader, conditions, stand\n",
    "cols_to_targ = [] # all other object columns\n",
    "\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype in ['float64', 'int64'] and c not in ['game_num', 'batter', 'fielder_2', 'umpire']:\n",
    "        num_cols.append(c)\n",
    "    elif len(X_train[c].unique()) < 10:\n",
    "        cols_to_ohe.append(c)\n",
    "    else:\n",
    "        cols_to_targ.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1588eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "ohe_transformer = Pipeline(steps=[\n",
    "    ('ohe_imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('ohencoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "target_encoder = Pipeline(steps=[\n",
    "    ('freq_enc', ce.target_encoder.TargetEncoder()),\n",
    "    ('freq_imputer', SimpleImputer(strategy='constant', fill_value=0))\n",
    "    \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('ohe', ohe_transformer, cols_to_ohe),\n",
    "        ('target', target_encoder, cols_to_targ)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f68256",
   "metadata": {},
   "source": [
    "# Baseline - Simple models\n",
    "\n",
    "For this project it wouldn't have been useful to simply normalize the number of hits against non-hits. My goal isn't simply to identify if a situation will result in a hit but rather what batter is the most likely to get a hit on a given day. So i devised a number of different simple models to compare my more advanced models against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47650a6",
   "metadata": {},
   "source": [
    "Pick player with the highest Plate Appearances per Game over the last 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b407f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(sample, output=True, metric='PA/G_batter')\n",
    "streak_checker(holdout, output=True, metric='PA/G_batter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30dbb0f",
   "metadata": {},
   "source": [
    "Observations - this seems to trend towards a small group of players except for when a player makes their mLB debut towards the top of the lineup (Wander Franco, Greg Deichmann). Sample size is an issue - could be addressed by either looking at games played or excluding PA/G above 4.9.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530de77",
   "metadata": {},
   "source": [
    "Pick player with highest launch speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(sample, output=True, metric='avg_launch_speed')\n",
    "streak_checker(holdout, output=True, metric='avg_launch_speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba8de7",
   "metadata": {},
   "source": [
    "The metrics point to this being mostly useless which is backed up by the fact that it seems to occasionally choose pitchers (Michael Pineda, Touki Toussaint), when it's not fawning over John Donaldson. Could be worth excluding pitchers as a class though would be hard to implement with the current dataset, not to mention the fact that 2-way players would need to be excluded from that filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afdc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(sample, output=True, metric=['avg_launch_angle', 'avg_launch_speed'], combiner='add')\n",
    "streak_checker(holdout, output=True, metric=['avg_launch_angle', 'avg_launch_speed'], combiner='add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527c8be",
   "metadata": {},
   "source": [
    "Exaserbates bias towards small sample sizes, leading part time players to dominate selection rather than proven hitters\n",
    "\n",
    "Would also be worth checking lefty righty splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b928502",
   "metadata": {},
   "outputs": [],
   "source": [
    "streak_checker(sample, output=True, metric='L/R_split', combiner='add')\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "streak_checker(holdout, output=True, metric='L/R_split', combiner='add')\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "streak_checker(sample, output=True, metric='L/R_split', combiner='multiply')\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "streak_checker(holdout, output=True, metric='L/R_split', combiner='multiply')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dfa466",
   "metadata": {},
   "source": [
    "Nothing stands out in these groups. Seems to be a mix of journeymen and all-stars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35babd76",
   "metadata": {},
   "source": [
    "# Model 1 - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51a6ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('tree', dt)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002d08c",
   "metadata": {},
   "source": [
    "# Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dec5a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "logreg = LogisticRegression(solver='sag')\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logreg', logreg)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5995af0",
   "metadata": {},
   "source": [
    "# Model 3: SVC too slow, linearSVC doesn't supply probability, so I need to supply a secondary way to select a single batter per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fe5b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "Linearsvc = LinearSVC()\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('linearsvc', Linearsvc)\n",
    "])\n",
    "\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e71cef",
   "metadata": {},
   "source": [
    "# Model 4: KNN - 5 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2835ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', knn)\n",
    "])\n",
    "\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e21a3",
   "metadata": {},
   "source": [
    "# Model 4: KNN - 3 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da280bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', knn)\n",
    "])\n",
    "\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0af7b0",
   "metadata": {},
   "source": [
    "# Model 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0deca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rfc', rfc)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213c0f6",
   "metadata": {},
   "source": [
    "# Model 6: AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6dc14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ada', ada)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac088e8c",
   "metadata": {},
   "source": [
    "# Model 7: GradientBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47677526",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('gbm', gbm)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc36b5",
   "metadata": {},
   "source": [
    "# Model 8: XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86840459",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "xbg = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xbg', xbg)\n",
    "])\n",
    "\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2148fc2",
   "metadata": {},
   "source": [
    "# Model 9: Random Forest with Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b1ef0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "parameters = {\n",
    "    'min_samples_split': [3, 5, 100], \n",
    "    'n_estimators' : [100, 300],\n",
    "        'max_depth': [3, 5, 15, 25],\n",
    "\n",
    "}\n",
    "\n",
    "estimator = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid = GridSearchCV(estimator, parameters, n_jobs=-1, cv=5, scoring = 'precision')\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('grid', grid)\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(datetime.now()-start)\n",
    "\n",
    "estimator = RandomForestClassifier(n_jobs=-1).set_params(grid.best_params_)\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('grid', estimator)\n",
    "])\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "run_model(clf, X_train, X_val, y_train, y_val, holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deaa422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ab7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
